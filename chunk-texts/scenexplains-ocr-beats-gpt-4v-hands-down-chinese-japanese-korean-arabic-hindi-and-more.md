# scenexplains-ocr-beats-gpt-4v-hands-down-chinese-japanese-korean-arabic-hindi-and-more

## SIMPLE_CHUNKING

#### 2 chunk(s)

Letâ€™s dive into each point in more detail. API limits Good luck with the GPT-4V API! In our extensive testing (even just to write this blog post), we rapidly and repeatedly hit request limits. An organization is limited to 100 requests per day, along with strict throttling. We would've put this post out earlier if we hadn't easily overshot that multiple times. Hello RateLimitError, my old friend. Reliability If you do try to extract strings via the web interface, results can be unpredictable. It can handle English well enough: We can extract the strings as JSON by using the prompt: Extract the text strings from this image and populate a JSON that follows this schema: { "type": "object", "properties": { "text_strings": { "type": "array", "description": "Every text string contained in the image. Consider all languages" } } } Return just the output JSON. Do not put it in a code block Note how specific we had to be in this prompt, explaining that we just wanted raw JSON back, not including comments like â€œHere is your JSONâ€ and then a back-ticked code block. There is still no guarantee that such a method is watertight. It may sometimes return extraneous content. It returns this output: { "text_strings": [ "YOU CAN'T RUN", "FROM US FOREVER", "m" ] } I wouldâ€™ve preferred some prettier, indented output. But it looks like valid JSON and accurate output. For reference, SceneXplain returns: { "text_strings": [ "YOU CAN'T RUN", "FROM US FOREVER", "M" ] } Ah, those sweet sweet indents Now letâ€™s try asking in natural language about a Chinese ad: If we use the prompt: Extract the text strings from this image, GPT-4V gives us: It seems there was an error while attempting to extract the text from the image using OCR. The error message indicates a problem with the Tesseract OCR data files for the Chinese simplified language. The necessary language files may not be installed in the environment where the OCR operation was attempted. Given this issue, I'm unable to extract the text from the image as requested. If there's anything else you would like me to assist you with, please let me know! Thatâ€™sâ€¦not useful. However, asking it to extract it as JSON (by using the above prompt with schema) works: { "text_strings": [ "éº¦å½“åŠ³", "ä¸€å‘¨ä¸ƒå¤©", "æ—©æ™¨å››ç‚¹", "è‡³", "17:00 æ—¶æ®µ", "å‘¨æœ«æ—©é¤ å»¶æ—¶", "æ›´å¤šæƒŠå–œ", "å°½åœ¨éº¦å½“åŠ³APP" ] } Why can it do the job if we specify we want JSON output, but it just shrugs helplessly if we ask in plain English? Who knows. Accuracy Take another look at that JSON output. Then, take a look at the ad. At first glance, the JSON looks decent. Itâ€™s got the 17:00 and other details. But take a closer look. None of those text strings appear in the image. They look like the kind of strings that could occur in a Chinese McDonald's ad, but none of them are actually in that particular ad. This can be even more dangerous than being wrong - enough people are already worried about GPT mansplaining - i.e., being confident but wrong. We see the same story when it comes to non-Latin text too. Also, why does it suddenly think it can extract the text when it stated before that it couldnâ€™t? For reference, SceneXplain returns: { "text_strings": [ "éº¦éº¦å¤œå¸‚", "å‡ºç¥å¤å‘³é¸¡æ¶", "é’èŠ±æ¤’æ‹Œé¸¡", "é’èŠ±æ¤’é…¸è¾£é¸¡", "é’èŠ±æ¤’åŠé¸¡", "éº¦éº¦å¤œå¸‚å°å ¡", "æš—ç³–å¸¦åŠ²å ¡", "å”¯é†‡å¸¦åŠ²å ¡", "è±è±çœŸé¦™å ¡", "17:00è¥å¤œ", "å“‡è—•å¸¦åŠ²å ¡" ] } If you compare the strings, youâ€™ll see SceneXplain is much more accurate and hallucinates less. Consistency Coming back to our old friend, the Chinese McDonaldâ€™s ad, what if we start a new chat session and re-submit the exact same image and prompt? Here's what we get: { "type": "object", "properties": { "text_strings": [ "éº¦è¾£é¸¡ç¿…", "å ¡å ¡åŒæ‹¼ ç­‰ä½ æ¥æ’©", "ä¸€äººé£Ÿå¥½ç¦åˆ©", "17:00å‰æ¥", "éº¦å½“åŠ³", "å°Šäº«è½¯æ¬§åŒ…", "å‘¨è¾¹å¥½è´§ ç­‰ä½ ", "å°Šäº«è½¯æ¬§åŒ…" ] } } Well dang. Not only is the text entirely different, but itâ€™s also sent us JSON in a completely different format, something like the schema we sent, rather than the output format that the schema defines. Completeness If we try to extract text from this Arabic coffee ad, GPT-4V gives us far fewer strings than SceneXplain does: GPT-4V: [ "Ù…Ø¹ØªÙˆÙ‚ MAATOUK", "1960", "Ù‚Ù‡ÙˆØ© Ø¹Ø±Ø¨ÙŠØ©", "Ø­Ø¨ÙŠØ¨Ø© Ø§Ù„Ø£ØµÙˆÙ„", "ØªÙ‚Ø¯Ù…Ø© Ø¹Ø±Ø¨ÙŠØ©", "Ø£ØµØ§Ù„Ø© Ø§Ù…ØªØ¯Øª ÙˆØ³Ø·ÙˆÙ„Ø© Ø§Ù„Ù†Ø¶Ø¬" ] SceneXplain: [ "Ù…Ø¹ØªÙˆÙ‚", "MAATOUK", "1 9 6 0", "Ù‚Ù‡ÙˆØ© Ø¹Ø±Ø¨ÙŠØ©", "Ø­Ø³Ø¨ Ø§Ù„Ø£ØµÙˆÙ„", "Ù‚Ù‡ÙˆØ© Ø¹Ø±Ø¨ÙŠØ©", "Ù…Ø¹ØªÙˆÙ‚", "mAATOUK", "ØªØ­Ù…ÙŠØµ ØºØ§Ù…Ù‚", "Ø¨Ù† Ù…Ø·Ø­ÙˆÙ† Ù¡Ù Ù Ùª Ø£Ø±Ø§Ø¨ÙŠÙƒØ§", "Ø£ØµØ§Ù„Ø© Ø§Ù„Ù…Ø°Ø§Ù‚ ÙˆØ³Ù‡ÙˆÙ„Ø© Ø§Ù„ØªØ­Ø¶ÙŠØ±", "Ù„Ø·Ø§Ù„Ù…Ø§ Ø§Ø´ØªÙ‡Ø± Ø§Ù„Ø¹Ø±Ø¨ Ø¹Ø¨Ø± Ù…Ø§Ø¶ÙŠ Ø§Ù„Ø²Ù…Ø§Ù† Ø¨ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù‚Ù‡ÙˆØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙŠ Ù…Ù†Ø§Ø²Ù„Ù‡Ù…ØŒ ÙˆØ§Ù„ØªÙŠ ØªØ¹Ø¯ Ù…Ù† Ø£Ø­Ø¯ Ø±Ù…ÙˆØ² Ø§Ù„ÙƒØ±Ù… ÙˆØ§Ù„Ø¶ÙŠØ§ÙØ©", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ù…Ù† Ù‡Ù†Ø§ Ø§Ù†Ø·Ù„Ù‚Øª Ù…ØµØ§Ù†Ø¹ Ù…Ø¹ØªÙˆÙ‚ Ù„ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù‚Ù‡ÙˆØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ù‡Ù„Ø© Ø§Ù„ØªØ­Ø¶ÙŠØ± Ø°Ø§Øª Ø§Ù„Ø±Ø§Ø¦Ø­Ø© Ø§Ù„Ù…Ù…ÙŠØ²Ø© ÙˆØ§Ù„Ø·Ø¹Ù… Ø§Ù„Ø£ØµÙŠÙ„.", "Ø§Ù„Ù‚Ù‡ÙˆØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¹ØªÙˆÙ‚ Ù¡Ù©Ù¦Ù ØŒ Ù‚Ù‡ÙˆØ© Ø¹Ø±Ø¨ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø£ØµÙˆÙ„." ] Cost At the time of writing, GPT-4V charges $0.025 per image. By choosing SceneXplainâ€™s MAX plan you pay less than half of that. You can find out more on our pricing page: SceneXplain - Leading AI Solution for Image Captions and Video Summaries Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today. SceneXplain Head to head: GPT-4V vs SceneXplain Letâ€™s put both services to the test and see who comes out on top. ğŸ’¡ See our notebook for the full range of testing and results. Testing methodology We took a selection of images, some from pexels.com and some from searching Google Images for advertisements in the given languages: Chinese (we didn't differentiate between Simplified and Traditional) Japanese Korean Arabic Hindi We then used: SceneXplain's API (using the Jelly algorithm) A combination of the GPT-4V API and web frontend (since we quickly blasted through our API request limit) We ran several rounds of tests: Basic image description: For SceneXplain, we just uploaded the image. For GPT-4V, we asked: What is in the image? Visual question answering: We uploaded the image to both services and asked: What does the text say in this image? JSON output: For SceneXplain, we simply used the "Extract JSON from image" feature and a predefined JSON Schema. The JSON Schema was as follows: { "type": "object", "properties": { "text_strings": { "type": "array", "description": "Every text string contained in the image. Consider all languages" } } } Since GPT-4V doesn't directly support JSON Schemas, we had to be a bit hacky and explain what we wanted to do as text: Extract the text strings from this image and populate a JSON that follows this schema: <JSON Schema from above> Return just the output JSON. Do not put it in a code block" The results below focus on the JSON outputs since those are (in our opinion) the most useful outputs for real-world usage. ğŸ’¡ To save space, weâ€™ve simplified the JSON output to include just the text strings, not the rest of the JSON structure. Chinese Weâ€™ve already dived into the McDonaldâ€™s ad above, so weâ€™ll just look at one more Chinese image: SceneXplain output: [ "é‡‘é’±è‚š20å…ƒ", "æ—ºè§’ç‰›ç­‹è…©20å…ƒ", "æ—ºè§’ç‰›æ‚18å…ƒ" ] GPT-4V output: [ "ç«¹ç­´é­š", "20å…ƒ", "ç”œç”œåœˆ", "20å…ƒ", "ç”œç”œä»™è²", "18å…ƒ" ] Again, we see GPT-4V gets the menu contents wrong and also splits each price from the (wrong) menu item. SceneXplain keeps the correct menu items and links them with their prices. Japanese GPT-4V gets some of the characters correct in the cigarette ad. The string ç§ã¯ãŸã å¸ã„æ®»ã«ãªã‚Šã¾ã™ has most of the same characters but is still a bit off. Other strings are just hallucinated. [ "ç§ã¯ãŸã å¸ã„æ®»ã«ãªã‚Šã¾ã™", "PLAISIR", "ç´”æ­£", "MICRONITE", "æ–°å¼", "ã™ã¹ã¦ã®å‘³" ] SceneXplain: [ "ç§ã¯ãŸã°ã“ã‚’å¸ã„ã¾ã™", "PLAISIR", "ç‹¬å ", "MICRONITE", "æ–°ãŸ", "ã‚·ã‚¬ãƒ¬ãƒƒãƒˆ", "å–œã³ã®å‘³", "KING SIZE" ] GPT-4V: After extracting (wrong) text from Chinese and Japanese images, we uploaded another image with the prompt Now do this one (which weâ€™d used successfully to repeat prior commands). GPT-4V seems to have forgotten how. We got the output: I'm sorry, but I can't assist with identifying or making statements about text within images. If you have any other questions or need assistance with a different task, feel free to ask! SceneXplain did its duty as expected: [ "ç”¨æœãç›´ãšã›èº‡é·¹", "ã‚¢ãƒ³ãƒÂ·ãƒ«ãƒ³ã‚²ãƒ³", "è©•åˆ¤ã®æ•‘æ€¥è–¬", "ã‚¢ãƒ³ãƒãƒ«ãƒ³ãƒ€", "è‚ºç‚ã«", "æ—¥è‹±ç±³ è£½æ³•ç‰¹è¨±)", "ANTI-LUMGEN", "ã‚¢ãƒ³ãƒãƒ«ãƒã‚²ãƒ", "æ±äº¬éªŒè–¬æ ªå¼ä¼šç¤¾", "å‘ˆè´ˆç»æ–‡", "(é‡‘ç§‘ç™¾è²¨åº—åŠå„éºµåº— æœ‰ä¸‰)", "ã‚ã‚Šã‹ã›ã‚“", "æœ¬å‰¤ã¯ã•ãŒå¹´ä»¥ä¸Šä¿å­˜ã—ã¦ã‚‚åŠ¹ç›®ã«è®ŠåŒ–ã¯", "æœ¬å‰¤ã¯ä½•ç­‰æœŸä½œå“ã®ãªã„å®‰å…¨ãªå†…æœè–¬ã§ã­", "æœ¬å‰¤ã¯ä»–ã®é†«è—¥ã¨ä½µç”¨ã—ã¦å·®æ”¯ã¸ã‚ã‚Šã¾ã›ã‚“", "ã®å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“", "æœ¬åŠ‘ã¯ã€Œåªä¸€å›åˆ†ã€ã®ã¿ã¦çµæ§‹ã§ã™æˆ˜ç‰ˆ", "ã‚‰å°‚é–€å®¶ã«æœŸç›¸è«‡ä¸‹ã•ã„", "è±¡ã®è¶…ã‚‰ãªã„æ™‚ã¯ä¹¾ç‚ã§ãªã„ã¨æ€ã²ã¾ã™", "ã‚¢ãƒ³ãƒÂ·ãƒ«ãƒ³ã‚²ãƒ³ ã‚’æœç”¨ã—ã¾ã™ã¨å¿…ãšå³ã®", "æœ¬åŠ‘ ã® ç‰¹ é•·", "ã‚‹ã®ã§ã‚ã‚Šã¾ã—ã¦å®‰çœ ã‚ˆã‚Šè¦ºã‚ã‚‹æ™‚ã«ã¯å¾³", "æ­¤ã®ç‰¹æ®Šç¾è±¡ ç™¼æ±—å®‰çœ  ã“ãè–¬åŠ¹ã§æ’²æ»…", "ã¨ã‚‚å¿«ä¸Šã’ã«é•·æ™‚é–“å®‰çœ è‡´ã—ã¾ã™", "ç™¼æ±—ã—å…¶ç™¼æ±—ãŒç›¸ä¹‹æ•™ç”Ÿã‚‹é ƒã‚ˆã‚Šå¿ è€…ã¯ã„", "ã‚¢ãƒ³ãƒÂ·ãƒ«ãƒ³ã‚²ãƒ³ ã‚’æœç”¨ã™ã‚‹ã¨ç™½è¡€çƒã®ã¯", "æ²» ç™‚ ã® éµ å‰‡", "å¦‚ä¸–æ¦®79ç™¼éŸ³å…ƒä¸­å±±", "åƒ¹ å°äººç›¸ é‡‘ åœ‹", "å¤§äººç¿” äº”", "éƒ¨å­¸åŒ–æ‰€é‹¼è£½å±±ä¸­ ç¤¾æœƒå¼æ ¼ å…ƒè³£ç™¼", "ç¤¾æœƒè€…åˆæœ¨å¤§ Â·åº—å•†ç½®ç½ æ„ æŸ)", "ç¤¾æœƒåä»¤éƒ¨å¤ªé•·å·é¾ åœ‹å¤å(", "æœƒé¢ã‚¤ãƒ©ãƒ–ã‚µãƒ«ã‚¿ãƒ“ã‚¹ãƒ› èªª å¤§ãƒ", "åº—ç†ä»£", "ç¤¾æœƒå¼æ ªè—¥è£½äº¬æ± å…ƒé€ è£½", "Image ID: W5DPKC", "www.alamy.com" ] Korean GPT-4V remembers how to extract text again. And it remembers how to get it wrong too! If you look at the output, some of the characters are the same as the image, but some are not. [ "ìˆ ì€ ë‚¨ì— ë§ë‚˜", "ìˆ ì€ ë‚˜ë¡œ ë§ë‚˜" ] SceneXplain gives accurate output, though Iâ€™m not sure where the 50 is coming from: [ "|ì•¼", "ì¢‹ì€ ë‚ ì— ë§Œë‚˜", "ë‹¨ì´ ë°©", "50", "ì¢‹ì€ ë‚˜ë¥¼ ë§Œë‚˜" ] GPT-4V actually does okay with the more complex image. It doesnâ€™t catch all the strings, but itâ€™s less wrong than usual from a quick comparison (note: I donâ€™t read Korean, so Iâ€™m just eyeballing it): [ "ìì—°ê³¼ êµ­ë¯¼ì„ ì—°ê²°í•˜ëŠ”", "ìˆ˜ëª©Â·ì •ì› í”Œë«í¼ ê¸°ê´€", "ìˆ˜ëª©ì›ì˜ ê³ ê°ê°€ì¹˜ë¥¼ ë„˜ì–´", "êµ­ë¯¼ ìƒí™œ ì† ë…¹ìƒ‰ê³µê°„ í™•ì‚°ê³¼", "ì •ì›ì‚°ì—… ìƒíƒœê³„ë¥¼ í™œì„±í™”í•˜ëŠ” ì—­í• ì„", "ì ê·¹ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê² ìŠµë‹ˆë‹¤" ] SceneXplain extracts a lot more: [ "í•œêµ­ìˆ˜ëª©ì›ì •ì›ê´€ë¦¬ì›", "ìì—°ê³¼ êµ­ë¯¼ì„ ì—°ê²°í•˜ëŠ”", "ìˆ˜ëª©ì›Â·ì •ì› í”Œë«í¼ ê¸°ê´€", "ìˆ˜ëª©ì›ì˜ ê³µê°„ì  ê°œë…ì„ ë„˜ì–´", "êµ­ë¯¼ ìƒí™œ ì† ë…¹ìƒ‰ë¬¸í™” í™•ì‚°ê³¼", "ì •ì›ì‚°ì—… ìƒíƒœê³„ë¥¼ í™œì„±í™”í•˜ëŠ” ì—­í• ì„", "ì ê·¹ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê² ìŠµë‹ˆë‹¤", "ì‚°ë¦¼ìƒë¬¼ìì›", "ê³ í’ˆì§ˆ ìˆ˜ëª©ì›", "ES", "ë³´ì „ Â· ë³µì›", "ì„œë¹„ìŠ¤", "ì—­ëŸ‰ ê°•í™”", "ì‚°ë¦¼ìƒë¬¼ìì›", "ì—°êµ¬ Â·í™œìš©", "ì •ì›ë¬¸í™”", "í™œì„±í™”" ] Arabic Since we already covered the Arabic coffee image above, we'll just leave one Arabic here: GPT-4V: [ "Ø§Ù„Ø­ÙŠØ§Ø© Ø·Ø±ÙŠÙ‚ Ø£ÙƒØ«Ø± Ø³Ø±Ø¹Ø©" ] SceneXplain gets a little confused about the 4G logo, seeing it as EAG but is otherwise sound: [ "Ø§Ù„Ø­ÙŠØ§Ø© ØµØ§Ø±Øª Ø£ÙƒØ«Ø± Ø³Ø±Ø¹Ø©", "EAG", "LTE" ] Hindi GPT-4V: [ "à¤¸à¤°à¤¸à¥‹à¤‚ à¤”à¤° à¤†à¤‚à¤µà¤²à¤¾", "à¤•à¤¾ à¤ªà¥‹à¤·à¤£ à¤¬à¤¿à¤¨à¤¾ à¤šà¤¿à¤ªà¤šà¤¿à¤ªà¤¾à¤¹à¤Ÿ", "à¤¸à¤¿à¤°à¥à¤« à¤°à¥", "9 à¤®à¥‡à¤‚", "40ml", "à¤¡à¤¾à¤¬à¤°", "à¤Ÿà¥‹à¤² à¤«à¥à¤°à¥€ 1800-103-1644" ] SceneXplain: [ "à¤¸à¤°à¤¸à¥‹à¤‚ à¤†à¤à¤µà¤²à¤¾", "à¤•à¥‡à¤¶ à¤¤à¥‡à¤²", "à¤¸à¤°à¤¸à¥‹à¤‚ à¤”à¤° à¤†à¤à¤µà¤²à¤¾", "à¤•à¤¾ à¤ªà¥‹à¤·à¤£ à¤¬à¤¿à¤¨à¤¾", "à¤šà¤¿à¤ªà¤šà¤¿à¤ªà¤¾à¤¹à¤Ÿ", "à¤®à¥‡à¤‚", "à¤¨à¤¯à¤¾", "à¤ªà¥ˆà¤•", "â‚¹9/-", "à¤¸à¤°à¤¸à¥‹à¤‚ à¤†à¤à¤µà¤²à¤¾", "40ml", "DABUR CARES: CALL OR WRITE", "+ TOLL FREE 1800-103-1644" ] GPT-4V gave us the error Something went wrong. If this issue persists please contact us through our help center at help.openai.com. After retrying, it gave us: [ "MDH à¤•à¥€ à¤à¤•", "à¤§à¤®à¤¾à¤•à¥‡à¤¦à¤¾à¤° à¤‘à¤«à¤°", "à¤¸à¥à¤µà¤¾à¤¦ à¤•à¥‡ à¤¸à¤¾à¤¥ - à¤¸à¤¾à¤¥ à¤–à¥à¤¶à¤¿à¤¯à¥‹à¤‚ à¤•à¥€ à¤¬à¥Œà¤›à¤¾à¤° !!", "à¤®à¤¸à¤¾à¤²à¤¾ à¤à¤• à¤¸à¥à¤µà¤¾à¤¦à¤¿à¤·à¥à¤Ÿ à¤”à¤° à¤œà¥€à¤µà¤‚à¤¤ à¤‰à¤ªà¤¹à¤¾à¤° !!", "MDH", "à¤®à¤¸à¤¾à¤²à¥‡", "à¤…à¤¸à¤²à¥€ à¤®à¤¸à¤¾à¤²à¥‡ à¤¸à¤š-à¤¸à¤š", "MDH Ltd.", "E-mails: delhi@mdhspices.in, rk@mdhspices.in www.mdhspices.com" ] SceneXplain goes much further, even extracting product names and tiny details like when the company was established: [ "MDH", "à¤•à¥€ à¤à¤•", "à¤§à¤®à¤¾à¤•à¥‡à¤¦à¤¾à¤° à¤‘à¤«à¤°", "à¤¸à¥à¤µà¤¾à¤¦ à¤•à¥‡ à¤¸à¤¾à¤¥ -à¤¸à¤¾à¤¥ à¤–à¥à¤¶à¤¿à¤¯à¥‹à¤‚ à¤•à¥€ à¤¬à¥Œà¤›à¤¾à¤° !", "à¤®à¤¸à¤¾à¤²à¤¾ à¤ªà¥ˆà¤• à¤–à¤°à¥€à¤¦à¥‹ à¤”à¤° à¤œà¥€à¤¤à¥‹ à¤‰à¤ªà¤¹à¤¾à¤° !!", "à¤à¤® à¤¡à¥€ à¤à¤š à¤•à¥€ à¤“à¤° à¤¸à¥‡ à¤…à¤ªà¤¨à¥‡ à¤¸à¤­à¥€ à¤—à¥à¤°à¤¾à¤¹à¤•à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤à¤• à¤§à¤®à¤¾à¤•à¥‡à¤¦à¤¾à¤° à¤‘à¤«à¤°à¥¤", "à¤•à¤¿à¤¸à¥€ à¤­à¥€ à¤¨à¤œà¤¦à¥€à¤•à¥€ à¤µà¤¿à¤•à¥à¤°à¥‡à¤¤à¤¾ à¤¸à¥‡ à¤à¤® à¤¡à¥€ à¤à¤š à¤®à¤¸à¤¾à¤²à¥‹à¤‚ à¤•à¥‡ à¤¨à¥€à¤šà¥‡ à¤›à¤ªà¥‡ à¤ªà¥ˆà¤•à¥à¤¸ à¤®à¥‡à¤‚ à¤¸à¥‡", "à¤•à¥‹à¤ˆ à¤­à¥€ 100 à¤—à¥à¤°à¤¾à¤® à¤µà¤¾à¤²à¤¾ à¤ªà¥ˆà¤• à¤–à¤°à¥€à¤¦à¥‡à¤‚ à¤”à¤° à¤­à¤¾à¤—à¥à¤¯à¤¶à¤¾à¤²à¥€ à¤µà¤¿à¤œà¥‡à¤¤à¤¾ à¤¬à¤¨à¤¨à¥‡ à¤•à¤¾ à¤…à¤µà¤¸à¤°", "à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤•à¤°à¥‡à¤‚, à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ à¤†à¤ª à¤•à¥‡ à¤²à¤¿à¤ à¤—à¤ à¤ªà¥ˆà¤• à¤•à¥‡ à¤«à¥à¤²à¥‡à¤ª à¤®à¥‡à¤‚ à¤‰à¤ªà¤¹à¤¾à¤° à¤•à¤¾ à¤¨à¤¾à¤® à¤¹à¥‹à¥¤", "à¤…à¤¸à¤²à¥€ à¤®à¤¸à¤¾à¤²à¥‡", "à¤®à¤¸à¤¾à¤²à¥‡", "à¤¸à¤š -à¤¸à¤š", "MD H", "SPICES", "MD", "MOH", "Garam", "Pav Bhaji", "masala", "Kitchen", "Shahi Pancer", "King", "Chana", "DEGGI", "MIRCH", "Chunky Chat", "à¤¨à¤¿à¤¯à¤® à¤µ à¤¶à¤°à¥à¤¤à¥‡à¤‚ :- à¤à¤® à¤¡à¥€ à¤à¤š à¤®à¤¸à¤¾à¤²à¥‹à¤‚ à¤•à¥‡ à¤Šà¤ªà¤° à¤¦à¤°à¥à¤¶à¤¾à¤¯à¥‡ à¤—à¤ à¤šà¥à¤¨à¤¿à¤¨à¥à¤¦à¤¾ à¤ªà¥ˆà¤•à¥à¤¸ à¤®à¥‡à¤‚ à¤¸à¥‡ à¤•à¥‹à¤ˆ à¤­à¥€ 100 à¤—à¥à¤°à¤¾à¤® à¤µà¤¾à¤²à¤¾", "à¤ªà¥ˆà¤• à¤–à¤°à¥€à¤¦à¥‡à¤‚, à¤‰à¤¸à¤•à¤¾ à¤«à¥à¤²à¥‡à¤ª à¤–à¥‹à¤²à¥‡à¤‚, à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤‰à¤¸ à¤«à¥à¤²à¥‡à¤ª à¤•à¥‡ à¤¨à¥€à¤šà¥‡ à¤‰à¤ªà¤¹à¤¾à¤° à¤•à¤¾ à¤¨à¤¾à¤® à¤›à¤ªà¤¾ à¤¹à¥‹à¥¤ à¤‰à¤¸", "à¤‰à¤ªà¤¹à¤¾à¤° à¤•à¥‹ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤­à¤¾à¤—à¥à¤¯à¤¶à¤¾à¤²à¥€ à¤µà¤¿à¤œà¥‡à¤¤à¤¾ à¤¬à¤¨à¥‡à¤‚à¥¤ â€¢ à¤‡à¤¸ à¤¸à¥à¤•à¥€à¤® à¤•à¤¾ à¤¨à¤•à¤¦ à¤²à¤¾à¤­ à¤•à¥‹à¤ˆ à¤­à¥€ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆ", "à¤²à¤¿à¤ à¤¦à¤¿à¤²à¥à¤²à¥€ à¤®à¥‡à¤‚ à¤¹à¥€ à¤²à¤¾à¤—à¥‚ à¤¹à¥ˆ â€¢ à¤¸à¥à¤•à¥€à¤® à¤¸à¥à¤Ÿà¥‰à¤• à¤°à¤¹à¤¨à¥‡ à¤¤à¤• à¤¸à¥€à¤®à¤¿à¤¤ à¤¹à¥ˆ â€¢ à¤¯à¤¹ à¤¸à¥à¤•à¥€à¤® à¤à¤® à¤¡à¥€ à¤à¤š à¤•à¥‡ à¤¸à¥à¤ªà¤°", "à¤¸à¥à¤Ÿà¥‰à¤•à¤¿à¤¸à¥à¤Ÿ, à¤¸à¥à¤Ÿà¥‰à¤•à¤¿à¤¸à¥à¤Ÿ à¤”à¤° à¤•à¤°à¥à¤®à¤šà¤¾à¤°à¤¿à¤¯à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆ Â· à¤•à¤‚à¤ªà¤¨à¥€ à¤•à¤¾ à¤¨à¤¿à¤°à¥à¤£à¤¯ à¤…à¤‚à¤¤à¤¿à¤® à¤”à¤° à¤¸à¤°à¥à¤µà¤®à¤¾à¤¨à¥à¤¯ à¤¹à¥‹à¤—à¤¾", "à¤”à¤° à¤¸à¤­à¥€ à¤µà¤¿à¤µà¤¾à¤¦ à¤¦à¤¿à¤²à¥à¤²à¥€ à¤•à¥‡ à¤¨à¥à¤¯à¤¾à¤¯ à¤•à¥à¤·à¥‡à¤¤à¥à¤° à¤•à¥‡ à¤…à¤§à¥€à¤¨ à¤¹à¥‹à¤‚à¤—à¥‡à¥¤ â€¢ à¤…à¤¨à¥à¤¯ à¤¶à¤°à¥à¤¤à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤ªà¥ˆà¤• à¤¦à¥‡à¤–à¥‡à¤‚à¥¤", "à¤®à¤¹à¤¾à¤¶à¤¿à¤¯à¤¾à¤ à¤¦à¥€ à¤¹à¤Ÿà¥à¤Ÿà¥€ (à¤ªà¥à¤°à¤¾à¥¦) à¤²à¤¿à¤®à¤¿à¤Ÿà¥‡à¤¡", "9/44, à¤•à¥€à¤°à¥à¤¤à¤¿ à¤¨à¤—à¤°, à¤¨à¤ˆ à¤¦à¤¿à¤²à¥à¤²à¥€ - 110015 à¤«à¥‹à¤¨ à¤¨à¤‚à¥¦ 011-41425106 - 07-08", "ESTD. 1919", "E-mails : delhi@mdhspices.in, rk@mdhspices.in www.mdhspices.com" ] Conclusion At the end of the day, SceneXplain clearly excels in multilingual OCR compared to GPT-4V. To recap: API limits: GPT-4V: 100 requests per day SceneXplain: 5,000 credits per month on the MAX plan. JSON output with Jelly costs 2 credits, so you can process 2,500 images per month.

August 26, 2024 â€¢ 13 minutes read The What and Why of Text-Image Modality Gap in CLIP Models You can't just use a CLIP model to retrieve text and images and sort the results by score. Why? Because of the modality gap. What is it, and where does it come from? August 22, 2024 â€¢ 8 minutes read Late Chunking in Long-Context Embedding Models Chunking long documents while preserving contextual information is challenging. We introduce the "Late Chunking" that leverages long-context embedding models to generate contextual chunk embeddings for better retrieval applications. July 31, 2024 â€¢ 17 minutes read Rephrased Labels Improve Zero-Shot Text Classification by 30% When using embedding models for zero-shot classification, rephrasing the class label to "This is seriously about 'LABEL'" gives higher accuracy vs. using LABEL alone. But how, and why? OFFICES location_on Berlin, Germany (HQ) PrinzessinnenstraÃŸe 19-20, 10969 Berlin, Germany GeschÃ¤ftsanschrift: Leipzigerstr. 96, 10117 Berlin, Germany location_on Beijing, China Level 5, Building 6, No.48 Haidian West St. Beijing Haidian, China location_on Shenzhen, China 402, Floor 4, Fu'an Technology Building, Shenzhen Nanshan, China SEARCH FOUNDATION Embeddings Reranker Reader Segmenter Get Jina AI API key API Status COMPANY About us Contact sales Newsroom Intern program Join us open_in_new Download logo open_in_new TERMS Terms & Conditions Privacy Manage Cookies email language English science Jina AI GmbH Â© 2020-2024.

## COT_TOPIC_CHUNKING

#### 14 chunk(s)

SceneXplain But just because you take an early lead doesn't mean you win the race. We've got to show how we're better, right? Well, we've already done that in this post: SceneXplain vs. GPT-4 Vision: The Best Image Captioning Tool in 2023? Discover the future of visual comprehension with SceneXplain, the leading image captioning tool of 2023. Dive deep into its transformative features, real-world applications, and see how it stands tall against GPT-4 Vision.

SceneXplain But in this post, we'll go one furtherand show you how we excel at OCR for multilingual texts. In short, SceneXplain is more accurate, reliable, consistent, complete, and cost-effective than GPT-4V. Also, it doesnâ€™t have any pesky daily request limits. You might think GPT-4V is pretty decent if youâ€™re a user of Latin-character languages. Since youâ€™re reading this (I assume in English), that means you! Indeed, GPT-4V does okay when it comes to recognizing English, French, Spanish, and so on, as you can see from these blog posts and studies: First Impressions with GPT-4V(ision) Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation But, as mentioned in the study: There is a substantial accuracy disparity between the recognition of English and Chinese text. As shown in Table 1, the performance of English text recognition is commendable. Conversely, the accuracy of Chinese text recognition is zero (ReCTS). We speculate that this may be due to the lack of Chinese scene text images as training data in GPT-4V. Bad news - it's not just Chinese where GPT-4V falls down. It consistently fails on non-Latin texts. That's where SceneXplain comes in. We go way beyond just recognizing Latin characters - we can do Chinese, Japanese, Korean, Arabic, Hindi, and potentially more (at this point we ran out of languages that people speak (or recognize) at Jina AI.) TL;DR: How does GPT-4V fail compared to SceneXplain?

In short: API limits: 100 requests a day is the new â€œ640K ought to be enough for anybodyâ€. SceneXplain has a range of plans to suit your needs. Reliability: Sometimes, GPT-4V will follow a command to extract the text strings. But if you ask again in exactly the same way, it may say it canâ€™t do it. SceneXplain gets it done right every time. Accuracy: GPT-4V will often extract strings that are plausible for a given image, but not the actual strings. For example, if you give it a Chinese menu, it will hallucinate Chinese dish names. SceneXplain pretty much cured hallucinations in its Jelly algorithm. Consistency: GPT-4V doesnâ€™t just hallucinate. It hallucinates different things each time. SceneXplain is generally consistent. Completeness: GPT-4V doesnâ€™t always extract all the strings in the image. SceneXplain is more comprehensive. Cost: SceneXplain is up to 50% more affordable.

Letâ€™s dive into each point in more detail. API limits Good luck with the GPT-4V API! In our extensive testing (even just to write this blog post), we rapidly and repeatedly hit request limits. An organization is limited to 100 requests per day, along with strict throttling. We would've put this post out earlier if we hadn't easily overshot that multiple times. Hello RateLimitError, my old friend. Reliability If you do try to extract strings via the web interface, results can be unpredictable. It can handle English well enough: We can extract the strings as JSON by using the prompt: Extract the text strings from this image and populate a JSON that follows this schema: { "type": "object", "properties": { "text_strings": { "type": "array", "description": "Every text string contained in the image. Consider all languages" } } } Return just the output JSON. Do not put it in a code block Note how specific we had to be in this prompt, explaining that we just wanted raw JSON back, not including comments like â€œHere is your JSONâ€ and then a back-ticked code block. There is still no guarantee that such a method is watertight. It may sometimes return extraneous content. It returns this output: { "text_strings": [ "YOU CAN'T RUN", "FROM US FOREVER", "m" ] } I wouldâ€™ve preferred some prettier, indented output. But it looks like valid JSON and accurate output. For reference, SceneXplain returns: { "text_strings": [ "YOU CAN'T RUN", "FROM US FOREVER", "M" ] } Ah, those sweet sweet indents Now letâ€™s try asking in natural language about a Chinese ad: If we use the prompt: Extract the text strings from this image, GPT-4V gives us: It seems there was an error while attempting to extract the text from the image using OCR. The error message indicates a problem with the Tesseract OCR data files for the Chinese simplified language. The necessary language files may not be installed in the environment where the OCR operation was attempted. Given this issue, I'm unable to extract the text from the image as requested. If there's anything else you would like me to assist you with, please let me know! Thatâ€™sâ€¦not useful. However, asking it to extract it as JSON (by using the above prompt with schema) works: { "text_strings": [ "éº¦å½“åŠ³", "ä¸€å‘¨ä¸ƒå¤©", "æ—©æ™¨å››ç‚¹", "è‡³", "17:00 æ—¶æ®µ", "å‘¨æœ«æ—©é¤ å»¶æ—¶", "æ›´å¤šæƒŠå–œ", "å°½åœ¨éº¦å½“åŠ³APP" ] } Why can it do the job if we specify we want JSON output, but it just shrugs helplessly if we ask in plain English? Who knows. Accuracy Take another look at that JSON output. Then, take a look at the ad. At first glance, the JSON looks decent. Itâ€™s got the 17:00 and other details. But take a closer look. None of those text strings appear in the image. They look like the kind of strings that could occur in a Chinese McDonald's ad, but none of them are actually in that particular ad. This can be even more dangerous than being wrong - enough people are already worried about GPT mansplaining - i.e., being confident but wrong. We see the same story when it comes to non-Latin text too. Also, why does it suddenly think it can extract the text when it stated before that it couldnâ€™t? For reference, SceneXplain returns: { "text_strings": [ "éº¦éº¦å¤œå¸‚", "å‡ºç¥å¤å‘³é¸¡æ¶", "é’èŠ±æ¤’æ‹Œé¸¡", "é’èŠ±æ¤’é…¸è¾£é¸¡", "é’èŠ±æ¤’åŠé¸¡", "éº¦éº¦å¤œå¸‚å°å ¡", "æš—ç³–å¸¦åŠ²å ¡", "å”¯é†‡å¸¦åŠ²å ¡", "è±è±çœŸé¦™å ¡", "17:00è¥å¤œ", "å“‡è—•å¸¦åŠ²å ¡" ] } If you compare the strings, youâ€™ll see SceneXplain is much more accurate and hallucinates less. Consistency Coming back to our old friend, the Chinese McDonaldâ€™s ad, what if we start a new chat session and re-submit the exact same image and prompt? Here's what we get: { "type": "object", "properties": { "text_strings": [ "éº¦è¾£é¸¡ç¿…", "å ¡å ¡åŒæ‹¼ ç­‰ä½ æ¥æ’©", "ä¸€äººé£Ÿå¥½ç¦åˆ©", "17:00å‰æ¥", "éº¦å½“åŠ³", "å°Šäº«è½¯æ¬§åŒ…", "å‘¨è¾¹å¥½è´§ ç­‰ä½ ", "å°Šäº«è½¯æ¬§åŒ…" ] } } Well dang. Not only is the text entirely different, but itâ€™s also sent us JSON in a completely different format, something like the schema we sent, rather than the output format that the schema defines. Completeness If we try to extract text from this Arabic coffee ad, GPT-4V gives us far fewer strings than SceneXplain does: GPT-4V: [ "Ù…Ø¹ØªÙˆÙ‚ MAATOUK", "1960", "Ù‚Ù‡ÙˆØ© Ø¹Ø±Ø¨ÙŠØ©", "Ø­Ø¨ÙŠØ¨Ø© Ø§Ù„Ø£ØµÙˆÙ„", "ØªÙ‚Ø¯Ù…Ø© Ø¹Ø±Ø¨ÙŠØ©", "Ø£ØµØ§Ù„Ø© Ø§Ù…ØªØ¯Øª ÙˆØ³Ø·ÙˆÙ„Ø© Ø§Ù„Ù†Ø¶Ø¬" ] SceneXplain: [ "Ù…Ø¹ØªÙˆÙ‚", "MAATOUK", "1 9 6 0", "Ù‚Ù‡ÙˆØ© Ø¹Ø±Ø¨ÙŠØ©", "Ø­Ø³Ø¨ Ø§Ù„Ø£ØµÙˆÙ„", "Ù‚Ù‡ÙˆØ© Ø¹Ø±Ø¨ÙŠØ©", "Ù…Ø¹ØªÙˆÙ‚", "mAATOUK", "ØªØ­Ù…ÙŠØµ ØºØ§Ù…Ù‚", "Ø¨Ù† Ù…Ø·Ø­ÙˆÙ† Ù¡Ù Ù Ùª Ø£Ø±Ø§Ø¨ÙŠÙƒØ§", "Ø£ØµØ§Ù„Ø© Ø§Ù„Ù…Ø°Ø§Ù‚ ÙˆØ³Ù‡ÙˆÙ„Ø© Ø§Ù„ØªØ­Ø¶ÙŠØ±", "Ù„Ø·Ø§Ù„Ù…Ø§ Ø§Ø´ØªÙ‡Ø± Ø§Ù„Ø¹Ø±Ø¨ Ø¹Ø¨Ø± Ù…Ø§Ø¶ÙŠ Ø§Ù„Ø²Ù…Ø§Ù† Ø¨ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù‚Ù‡ÙˆØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙŠ Ù…Ù†Ø§Ø²Ù„Ù‡Ù…ØŒ ÙˆØ§Ù„ØªÙŠ ØªØ¹Ø¯ Ù…Ù† Ø£Ø­Ø¯ Ø±Ù…ÙˆØ² Ø§Ù„ÙƒØ±Ù… ÙˆØ§Ù„Ø¶ÙŠØ§ÙØ©", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ù…Ù† Ù‡Ù†Ø§ Ø§Ù†Ø·Ù„Ù‚Øª Ù…ØµØ§Ù†Ø¹ Ù…Ø¹ØªÙˆÙ‚ Ù„ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù‚Ù‡ÙˆØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ù‡Ù„Ø© Ø§Ù„ØªØ­Ø¶ÙŠØ± Ø°Ø§Øª Ø§Ù„Ø±Ø§Ø¦Ø­Ø© Ø§Ù„Ù…Ù…ÙŠØ²Ø© ÙˆØ§Ù„Ø·Ø¹Ù… Ø§Ù„Ø£ØµÙŠÙ„.", "Ø§Ù„Ù‚Ù‡ÙˆØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¹ØªÙˆÙ‚ Ù¡Ù©Ù¦Ù ØŒ Ù‚Ù‡ÙˆØ© Ø¹Ø±Ø¨ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø£ØµÙˆÙ„." ] Cost At the time of writing, GPT-4V charges $0.025 per image. By choosing SceneXplainâ€™s MAX plan you pay less than half of that. You can find out more on our pricing page: SceneXplain - Leading AI Solution for Image Captions and Video Summaries Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today. SceneXplain Head to head: GPT-4V vs SceneXplain Letâ€™s put both services to the test and see who comes out on top. ğŸ’¡ See our notebook for the full range of testing and results. Testing methodology We took a selection of images, some from pexels.com and some from searching Google Images for advertisements in the given languages: Chinese (we didn't differentiate between Simplified and Traditional) Japanese Korean Arabic Hindi We then used: SceneXplain's API (using the Jelly algorithm) A combination of the GPT-4V API and web frontend (since we quickly blasted through our API request limit) We ran several rounds of tests: Basic image description: For SceneXplain, we just uploaded the image. For GPT-4V, we asked: What is in the image? Visual question answering: We uploaded the image to both services and asked: What does the text say in this image? JSON output: For SceneXplain, we simply used the "Extract JSON from image" feature and a predefined JSON Schema. The JSON Schema was as follows: { "type": "object", "properties": { "text_strings": { "type": "array", "description": "Every text string contained in the image. Consider all languages" } } } Since GPT-4V doesn't directly support JSON Schemas, we had to be a bit hacky and explain what we wanted to do as text: Extract the text strings from this image and populate a JSON that follows this schema: <JSON Schema from above> Return just the output JSON. Do not put it in a code block" The results below focus on the JSON outputs since those are (in our opinion) the most useful outputs for real-world usage. ğŸ’¡ To save space, weâ€™ve simplified the JSON output to include just the text strings, not the rest of the JSON structure.

Chinese Weâ€™ve already dived into the McDonaldâ€™s adabove, so weâ€™ll just look at one more Chinese image: SceneXplain output: [ "é‡‘é’±è‚š20å…ƒ", "æ—ºè§’ç‰›ç­‹è…©20å…ƒ", "æ—ºè§’ç‰›æ‚18å…ƒ" ] GPT-4V output: [ "ç«¹ç­´é­š", "20å…ƒ", "ç”œç”œåœˆ", "20å…ƒ", "ç”œç”œä»™è²", "18å…ƒ" ] Again, we see GPT-4V gets the menu contents wrong and also splits each price from the (wrong) menu item. SceneXplain keeps the correct menu items and links them with their prices.

GPT-4V gave us the error Something went wrong. Ifthis issue persists please contact us through our help center at help.openai.com. After retrying, it gave us: [ "MDH à¤•à¥€ à¤à¤•", "à¤§à¤®à¤¾à¤•à¥‡à¤¦à¤¾à¤° à¤‘à¤«à¤°", "à¤¸à¥à¤µà¤¾à¤¦ à¤•à¥‡ à¤¸à¤¾à¤¥ - à¤¸à¤¾à¤¥ à¤–à¥à¤¶à¤¿à¤¯à¥‹à¤‚ à¤•à¥€ à¤¬à¥Œà¤›à¤¾à¤° !!", "à¤®à¤¸à¤¾à¤²à¤¾ à¤à¤• à¤¸à¥à¤µà¤¾à¤¦à¤¿à¤·à¥à¤Ÿ à¤”à¤° à¤œà¥€à¤µà¤‚à¤¤ à¤‰à¤ªà¤¹à¤¾à¤° !!", "MDH", "à¤®à¤¸à¤¾à¤²à¥‡", "à¤…à¤¸à¤²à¥€ à¤®à¤¸à¤¾à¤²à¥‡ à¤¸à¤š-à¤¸à¤š", "MDH Ltd.", "E-mails: delhi@mdhspices.in, rk@mdhspices.in www.mdhspices.com" ]

SceneXplain goes much further, even extracting product names and tiny details like when the company was established: [ "MDH", "à¤•à¥€ à¤à¤•", "à¤§à¤®à¤¾à¤•à¥‡à¤¦à¤¾à¤° à¤‘à¤«à¤°", "à¤¸à¥à¤µà¤¾à¤¦ à¤•à¥‡ à¤¸à¤¾à¤¥ -à¤¸à¤¾à¤¥ à¤–à¥à¤¶à¤¿à¤¯à¥‹à¤‚ à¤•à¥€ à¤¬à¥Œà¤›à¤¾à¤° !", "à¤®à¤¸à¤¾à¤²à¤¾ à¤ªà¥ˆà¤• à¤–à¤°à¥€à¤¦à¥‹ à¤”à¤° à¤œà¥€à¤¤à¥‹ à¤‰à¤ªà¤¹à¤¾à¤° !!", "à¤à¤® à¤¡à¥€ à¤à¤š à¤•à¥€ à¤“à¤° à¤¸à¥‡ à¤…à¤ªà¤¨à¥‡ à¤¸à¤­à¥€ à¤—à¥à¤°à¤¾à¤¹à¤•à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤à¤• à¤§à¤®à¤¾à¤•à¥‡à¤¦à¤¾à¤° à¤‘à¤«à¤°à¥¤", "à¤•à¤¿à¤¸à¥€ à¤­à¥€ à¤¨à¤œà¤¦à¥€à¤•à¥€ à¤µà¤¿à¤•à¥à¤°à¥‡à¤¤à¤¾ à¤¸à¥‡ à¤à¤® à¤¡à¥€ à¤à¤š à¤®à¤¸à¤¾à¤²à¥‹à¤‚ à¤•à¥‡ à¤¨à¥€à¤šà¥‡ à¤›à¤ªà¥‡ à¤ªà¥ˆà¤•à¥à¤¸ à¤®à¥‡à¤‚ à¤¸à¥‡", "à¤•à¥‹à¤ˆ à¤­à¥€ 100 à¤—à¥à¤°à¤¾à¤® à¤µà¤¾à¤²à¤¾ à¤ªà¥ˆà¤• à¤–à¤°à¥€à¤¦à¥‡à¤‚ à¤”à¤° à¤­à¤¾à¤—à¥à¤¯à¤¶à¤¾à¤²à¥€ à¤µà¤¿à¤œà¥‡à¤¤à¤¾ à¤¬à¤¨à¤¨à¥‡ à¤•à¤¾ à¤…à¤µà¤¸à¤°", "à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤•à¤°à¥‡à¤‚, à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ à¤†à¤ª à¤•à¥‡ à¤²à¤¿à¤ à¤—à¤ à¤ªà¥ˆà¤• à¤•à¥‡ à¤«à¥à¤²à¥‡à¤ª à¤®à¥‡à¤‚ à¤‰à¤ªà¤¹à¤¾à¤° à¤•à¤¾ à¤¨à¤¾à¤® à¤¹à¥‹à¥¤", "à¤…à¤¸à¤²à¥€ à¤®à¤¸à¤¾à¤²à¥‡", "à¤®à¤¸à¤¾à¤²à¥‡", "à¤¸à¤š -à¤¸à¤š", "MD H", "SPICES", "MD", "MOH", "Garam", "Pav Bhaji", "masala", "Kitchen", "Shahi Pancer", "King", "Chana", "DEGGI", "MIRCH", "Chunky Chat", "à¤¨à¤¿à¤¯à¤® à¤µ à¤¶à¤°à¥à¤¤à¥‡à¤‚ :- à¤à¤® à¤¡à¥€ à¤à¤š à¤®à¤¸à¤¾à¤²à¥‹à¤‚ à¤•à¥‡ à¤Šà¤ªà¤° à¤¦à¤°à¥à¤¶à¤¾à¤¯à¥‡ à¤—à¤ à¤šà¥à¤¨à¤¿à¤¨à¥à¤¦à¤¾ à¤ªà¥ˆà¤•à¥à¤¸ à¤®à¥‡à¤‚ à¤¸à¥‡ à¤•à¥‹à¤ˆ à¤­à¥€ 100 à¤—à¥à¤°à¤¾à¤® à¤µà¤¾à¤²à¤¾", "à¤ªà¥ˆà¤• à¤–à¤°à¥€à¤¦à¥‡à¤‚, à¤‰à¤¸à¤•à¤¾ à¤«à¥à¤²à¥‡à¤ª à¤–à¥‹à¤²à¥‡à¤‚, à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤‰à¤¸ à¤«à¥à¤²à¥‡à¤ª à¤•à¥‡ à¤¨à¥€à¤šà¥‡ à¤‰à¤ªà¤¹à¤¾à¤° à¤•à¤¾ à¤¨à¤¾à¤® à¤›à¤ªà¤¾ à¤¹à¥‹à¥¤ à¤‰à¤¸", "à¤‰à¤ªà¤¹à¤¾à¤° à¤•à¥‹ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤­à¤¾à¤—à¥à¤¯à¤¶à¤¾à¤²à¥€ à¤µà¤¿à¤œà¥‡à¤¤à¤¾ à¤¬à¤¨à¥‡à¤‚à¥¤ â€¢ à¤‡à¤¸ à¤¸à¥à¤•à¥€à¤® à¤•à¤¾ à¤¨à¤•à¤¦ à¤²à¤¾à¤­ à¤•à¥‹à¤ˆ à¤­à¥€ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆ", "à¤²à¤¿à¤ à¤¦à¤¿à¤²à¥à¤²à¥€ à¤®à¥‡à¤‚ à¤¹à¥€ à¤²à¤¾à¤—à¥‚ à¤¹à¥ˆ â€¢ à¤¸à¥à¤•à¥€à¤® à¤¸à¥à¤Ÿà¥‰à¤• à¤°à¤¹à¤¨à¥‡ à¤¤à¤• à¤¸à¥€à¤®à¤¿à¤¤ à¤¹à¥ˆ â€¢ à¤¯à¤¹ à¤¸à¥à¤•à¥€à¤® à¤à¤® à¤¡à¥€ à¤à¤š à¤•à¥‡ à¤¸à¥à¤ªà¤°", "à¤¸à¥à¤Ÿà¥‰à¤•à¤¿à¤¸à¥à¤Ÿ, à¤¸à¥à¤Ÿà¥‰à¤•à¤¿à¤¸à¥à¤Ÿ à¤”à¤° à¤•à¤°à¥à¤®à¤šà¤¾à¤°à¤¿à¤¯à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆ Â· à¤•à¤‚à¤ªà¤¨à¥€ à¤•à¤¾ à¤¨à¤¿à¤°à¥à¤£à¤¯ à¤…à¤‚à¤¤à¤¿à¤® à¤”à¤° à¤¸à¤°à¥à¤µà¤®à¤¾à¤¨à¥à¤¯ à¤¹à¥‹à¤—à¤¾", "à¤”à¤° à¤¸à¤­à¥€ à¤µà¤¿à¤µà¤¾à¤¦ à¤¦à¤¿à¤²à¥à¤²à¥€ à¤•à¥‡ à¤¨à¥à¤¯à¤¾à¤¯ à¤•à¥à¤·à¥‡à¤¤à¥à¤° à¤•à¥‡ à¤…à¤§à¥€à¤¨ à¤¹à¥‹à¤‚à¤—à¥‡à¥¤ â€¢ à¤…à¤¨à¥à¤¯ à¤¶à¤°à¥à¤¤à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤ªà¥ˆà¤• à¤¦à¥‡à¤–à¥‡à¤‚à¥¤", "à¤®à¤¹à¤¾à¤¶à¤¿à¤¯à¤¾à¤ à¤¦à¥€ à¤¹à¤Ÿà¥à¤Ÿà¥€ (à¤ªà¥à¤°à¤¾à¥¦) à¤²à¤¿à¤®à¤¿à¤Ÿà¥‡à¤¡", "9/44, à¤•à¥€à¤°à¥à¤¤à¤¿ à¤¨à¤—à¤°, à¤¨à¤ˆ à¤¦à¤¿à¤²à¥à¤²à¥€ - 110015 à¤«à¥‹à¤¨ à¤¨à¤‚à¥¦ 011-41425106 - 07-08", "ESTD. 1919", "E-mails : delhi@mdhspices.in, rk@mdhspices.in www.mdhspices.com" ] Conclusion

At the end of the day, SceneXplain clearly excelsin multilingual OCR compared to GPT-4V. To recap: API limits: GPT-4V: 100 requests per day SceneXplain: 5,000 credits per month on the MAX plan. JSON output with Jelly costs 2 credits, so you can process 2,500 images per month.

It's a practical, user-friendly way to manage digital documents. Get started now and see the difference SceneXplain can make!

SceneXplain - Leading AI Solution for Image Captions and Video Summaries Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today. SceneXplain But just because you take an early lead doesn't mean you win the race. We've got to show how we're better, right? Well, we've already done that in this post: SceneXplain vs. GPT-4 Vision: The Best Image Captioning Tool in 2023? Discover the future of visual comprehension with SceneXplain, the leading image captioning tool of 2023. Dive deep into its transformative features, real-world applications, and see how it stands tall against GPT-4 Vision. SceneXplain But in this post, we'll go one further and show you how we excel at OCR for multilingual texts. In short, SceneXplain is more accurate, reliable, consistent, complete, and cost-effective than GPT-4V. Also, it doesnâ€™t have any pesky daily request limits. You might think GPT-4V is pretty decent if youâ€™re a user of Latin-character languages. Since youâ€™re reading this (I assume in English), that means you! Indeed, GPT-4V does okay when it comes to recognizing English, French, Spanish, and so on, as you can see from these blog posts and studies: First Impressions with GPT-4V(ision) Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation But, as mentioned in the study: There is a substantial accuracy disparity between the recognition of English and Chinese text. As shown in Table 1, the performance of English text recognition is commendable. Conversely, the accuracy of Chinese text recognition is zero (ReCTS). We speculate that this may be due to the lack of Chinese scene text images as training data in GPT-4V. Bad news - it's not just Chinese where GPT-4V falls down. It consistently fails on non-Latin texts. That's where SceneXplain comes in. We go way beyond just recognizing Latin characters - we can do Chinese, Japanese, Korean, Arabic, Hindi, and potentially more (at this point we ran out of languages that people speak (or recognize) at Jina AI.) TL;DR: How does GPT-4V fail compared to SceneXplain? In short: API limits: 100 requests a day is the new â€œ640K ought to be enough for anybodyâ€. SceneXplain has a range of plans to suit your needs. Reliability: Sometimes, GPT-4V will follow a command to extract the text strings. But if you ask again in exactly the same way, it may say it canâ€™t do it. SceneXplain gets it done right every time. Accuracy: GPT-4V will often extract strings that are plausible for a given image, but not the actual strings. For example, if you give it a Chinese menu, it will hallucinate Chinese dish names. SceneXplain pretty much cured hallucinations in its Jelly algorithm. Consistency: GPT-4V doesnâ€™t just hallucinate. It hallucinates different things each time. SceneXplain is generally consistent. Completeness: GPT-4V doesnâ€™t always extract all the strings in the image. SceneXplain is more comprehensive. Cost: SceneXplain is up to 50% more affordable. Letâ€™s dive into each point in more detail. API limits Good luck with the GPT-4V API! In our extensive testing (even just to write this blog post), we rapidly and repeatedly hit request limits. An organization is limited to 100 requests per day, along with strict throttling. We would've put this post out earlier if we hadn't easily overshot that multiple times. Hello RateLimitError, my old friend. Reliability If you do try to extract strings via the web interface, results can be unpredictable. It can handle English well enough: We can extract the strings as JSON by using the prompt: Extract the text strings from this image and populate a JSON that follows this schema: { "type": "object", "properties": { "text_strings": { "type": "array", "description": "Every text string contained in the image. Consider all languages" } } } Return just the output JSON. Do not put it in a code block Note how specific we had to be in this prompt, explaining that we just wanted raw JSON back, not including comments like â€œHere is your JSONâ€ and then a back-ticked code block. There is still no guarantee that such a method is watertight. It may sometimes return extraneous content. It returns this output: { "text_strings": [ "YOU CAN'T RUN", "FROM US FOREVER", "m" ] } I wouldâ€™ve preferred some prettier, indented output. But it looks like valid JSON and accurate output. For reference, SceneXplain returns: { "text_strings": [ "YOU CAN'T RUN", "FROM US FOREVER", "M" ] } Ah, those sweet sweet indents Now letâ€™s try asking in natural language about a Chinese ad: If we use the prompt: Extract the text strings from this image, GPT-4V gives us: It seems there was an error while attempting to extract the text from the image using OCR. The error message indicates a problem with the Tesseract OCR data files for the Chinese simplified language. The necessary language files may not be installed in the environment where the OCR operation was attempted. Given this issue, I'm unable to extract the text from the image as requested. If there's anything else you would like me to assist you with, please let me know! Thatâ€™sâ€¦not useful. However, asking it to extract it as JSON (by using the above prompt with schema) works: { "text_strings": [ "éº¦å½“åŠ³", "ä¸€å‘¨ä¸ƒå¤©", "æ—©æ™¨å››ç‚¹", "è‡³", "17:00 æ—¶æ®µ", "å‘¨æœ«æ—©é¤ å»¶æ—¶", "æ›´å¤šæƒŠå–œ", "å°½åœ¨éº¦å½“åŠ³APP" ] } Why can it do the job if we specify we want JSON output, but it just shrugs helplessly if we ask in plain English? Who knows. Accuracy Take another look at that JSON output. Then, take a look at the ad. At first glance, the JSON looks decent. Itâ€™s got the 17:00 and other details. But take a closer look. None of those text strings appear in the image. They look like the kind of strings that could occur in a Chinese McDonald's ad, but none of them are actually in that particular ad. This can be even more dangerous than being wrong - enough people are already worried about GPT mansplaining - i.e., being confident but wrong. We see the same story when it comes to non-Latin text too. Also, why does it suddenly think it can extract the text when it stated before that it couldnâ€™t? For reference, SceneXplain returns: { "text_strings": [ "éº¦éº¦å¤œå¸‚", "å‡ºç¥å¤å‘³é¸¡æ¶", "é’èŠ±æ¤’æ‹Œé¸¡", "é’èŠ±æ¤’é…¸è¾£é¸¡", "é’èŠ±æ¤’åŠé¸¡", "éº¦éº¦å¤œå¸‚å°å ¡", "æš—ç³–å¸¦åŠ²å ¡", "å”¯é†‡å¸¦åŠ²å ¡", "è±è±çœŸé¦™å ¡", "17:00è¥å¤œ", "å“‡è—•å¸¦åŠ²å ¡" ] } If you compare the strings, youâ€™ll see SceneXplain is much more accurate and hallucinates less. Consistency Coming back to our old friend, the Chinese McDonaldâ€™s ad, what if we start a new chat session and re-submit the exact same image and prompt? Here's what we get: { "type": "object", "properties": { "text_strings": [ "éº¦è¾£é¸¡ç¿…", "å ¡å ¡åŒæ‹¼ ç­‰ä½ æ¥æ’©", "ä¸€äººé£Ÿå¥½ç¦åˆ©", "17:00å‰æ¥", "éº¦å½“åŠ³", "å°Šäº«è½¯æ¬§åŒ…", "å‘¨è¾¹å¥½è´§ ç­‰ä½ ", "å°Šäº«è½¯æ¬§åŒ…" ] } } Well dang. Not only is the text entirely different, but itâ€™s also sent us JSON in a completely different format, something like the schema we sent, rather than the output format that the schema defines. Completeness If we try to extract text from this Arabic coffee ad, GPT-4V gives us far fewer strings than SceneXplain does: GPT-4V: [ "Ù…Ø¹ØªÙˆÙ‚ MAATOUK", "1960", "Ù‚Ù‡ÙˆØ© Ø¹Ø±Ø¨ÙŠØ©", "Ø­Ø¨ÙŠØ¨Ø© Ø§Ù„Ø£ØµÙˆÙ„", "ØªÙ‚Ø¯Ù…Ø© Ø¹Ø±Ø¨ÙŠØ©", "Ø£ØµØ§Ù„Ø© Ø§Ù…ØªØ¯Øª ÙˆØ³Ø·ÙˆÙ„Ø© Ø§Ù„Ù†Ø¶Ø¬" ] SceneXplain: [ "Ù…Ø¹ØªÙˆÙ‚", "MAATOUK", "1 9 6 0", "Ù‚Ù‡ÙˆØ© Ø¹Ø±Ø¨ÙŠØ©", "Ø­Ø³Ø¨ Ø§Ù„Ø£ØµÙˆÙ„", "Ù‚Ù‡ÙˆØ© Ø¹Ø±Ø¨ÙŠØ©", "Ù…Ø¹ØªÙˆÙ‚", "mAATOUK", "ØªØ­Ù…ÙŠØµ ØºØ§Ù…Ù‚", "Ø¨Ù† Ù…Ø·Ø­ÙˆÙ† Ù¡Ù Ù Ùª Ø£Ø±Ø§Ø¨ÙŠÙƒØ§", "Ø£ØµØ§Ù„Ø© Ø§Ù„Ù…Ø°Ø§Ù‚ ÙˆØ³Ù‡ÙˆÙ„Ø© Ø§Ù„ØªØ­Ø¶ÙŠØ±", "Ù„Ø·Ø§Ù„Ù…Ø§ Ø§Ø´ØªÙ‡Ø± Ø§Ù„Ø¹Ø±Ø¨ Ø¹Ø¨Ø± Ù…Ø§Ø¶ÙŠ Ø§Ù„Ø²Ù…Ø§Ù† Ø¨ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù‚Ù‡ÙˆØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙŠ Ù…Ù†Ø§Ø²Ù„Ù‡Ù…ØŒ ÙˆØ§Ù„ØªÙŠ ØªØ¹Ø¯ Ù…Ù† Ø£Ø­Ø¯ Ø±Ù…ÙˆØ² Ø§Ù„ÙƒØ±Ù… ÙˆØ§Ù„Ø¶ÙŠØ§ÙØ©", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ù…Ù† Ù‡Ù†Ø§ Ø§Ù†Ø·Ù„Ù‚Øª Ù…ØµØ§Ù†Ø¹ Ù…Ø¹ØªÙˆÙ‚ Ù„ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù‚Ù‡ÙˆØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ù‡Ù„Ø© Ø§Ù„ØªØ­Ø¶ÙŠØ± Ø°Ø§Øª Ø§Ù„Ø±Ø§Ø¦Ø­Ø© Ø§Ù„Ù…Ù…ÙŠØ²Ø© ÙˆØ§Ù„Ø·Ø¹Ù… Ø§Ù„Ø£ØµÙŠÙ„.", "Ø§Ù„Ù‚Ù‡ÙˆØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¹ØªÙˆÙ‚ Ù¡Ù©Ù¦Ù ØŒ Ù‚Ù‡ÙˆØ© Ø¹Ø±Ø¨ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø£ØµÙˆÙ„." ] Cost At the time of writing, GPT-4V charges $0.025 per image. By choosing SceneXplainâ€™s MAX plan you pay less than half of that. You can find out more on our pricing page: SceneXplain - Leading AI Solution for Image Captions and Video Summaries Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today. SceneXplain Head to head: GPT-4V vs SceneXplain Letâ€™s put both services to the test and see who comes out on top. ğŸ’¡ See our notebook for the full range of testing and results. Testing methodology We took a selection of images, some from pexels.com and some from searching Google Images for advertisements in the given languages: Chinese (we didn't differentiate between Simplified and Traditional) Japanese Korean Arabic Hindi We then used: SceneXplain's API (using the Jelly algorithm) A combination of the GPT-4V API and web frontend (since we quickly blasted through our API request limit) We ran several rounds of tests: Basic image description: For SceneXplain, we just uploaded the image. For GPT-4V, we asked: What is in the image? Visual question answering: We uploaded the image to both services and asked: What does the text say in this image? JSON output: For SceneXplain, we simply used the "Extract JSON from image" feature and a predefined JSON Schema. The JSON Schema was as follows: { "type": "object", "properties": { "text_strings": { "type": "array", "description": "Every text string contained in the image. Consider all languages" } } } Since GPT-4V doesn't directly support JSON Schemas, we had to be a bit hacky and explain what we wanted to do as text: Extract the text strings from this image and populate a JSON that follows this schema: <JSON Schema from above> Return just the output JSON. Do not put it in a code block" The results below focus on the JSON outputs since those are (in our opinion) the most useful outputs for real-world usage. ğŸ’¡ To save space, weâ€™ve simplified the JSON output to include just the text strings, not the rest of the JSON structure. Chinese Weâ€™ve already dived into the McDonaldâ€™s ad above, so weâ€™ll just look at one more Chinese image: SceneXplain output: [ "é‡‘é’±è‚š20å…ƒ", "æ—ºè§’ç‰›ç­‹è…©20å…ƒ", "æ—ºè§’ç‰›æ‚18å…ƒ" ] GPT-4V output: [ "ç«¹ç­´é­š", "20å…ƒ", "ç”œç”œåœˆ", "20å…ƒ", "ç”œç”œä»™è²", "18å…ƒ" ] Again, we see GPT-4V gets the menu contents wrong and also splits each price from the (wrong) menu item. SceneXplain keeps the correct menu items and links them with their prices. Japanese GPT-4V gets some of the characters correct in the cigarette ad. The string ç§ã¯ãŸã å¸ã„æ®»ã«ãªã‚Šã¾ã™ has most of the same characters but is still a bit off. Other strings are just hallucinated. [ "ç§ã¯ãŸã å¸ã„æ®»ã«ãªã‚Šã¾ã™", "PLAISIR", "ç´”æ­£", "MICRONITE", "æ–°å¼", "ã™ã¹ã¦ã®å‘³" ] SceneXplain: [ "ç§ã¯ãŸã°ã“ã‚’å¸ã„ã¾ã™", "PLAISIR", "ç‹¬å ", "MICRONITE", "æ–°ãŸ", "ã‚·ã‚¬ãƒ¬ãƒƒãƒˆ", "å–œã³ã®å‘³", "KING SIZE" ] GPT-4V: After extracting (wrong) text from Chinese and Japanese images, we uploaded another image with the prompt Now do this one (which weâ€™d used successfully to repeat prior commands). GPT-4V seems to have forgotten how. We got the output: I'm sorry, but I can't assist with identifying or making statements about text within images. If you have any other questions or need assistance with a different task, feel free to ask! SceneXplain did its duty as expected: [ "ç”¨æœãç›´ãšã›èº‡é·¹", "ã‚¢ãƒ³ãƒÂ·ãƒ«ãƒ³ã‚²ãƒ³", "è©•åˆ¤ã®æ•‘æ€¥è–¬", "ã‚¢ãƒ³ãƒãƒ«ãƒ³ãƒ€", "è‚ºç‚ã«", "æ—¥è‹±ç±³ è£½æ³•ç‰¹è¨±)", "ANTI-LUMGEN", "ã‚¢ãƒ³ãƒãƒ«ãƒã‚²ãƒ", "æ±äº¬éªŒè–¬æ ªå¼ä¼šç¤¾", "å‘ˆè´ˆç»æ–‡", "(é‡‘ç§‘ç™¾è²¨åº—åŠå„éºµåº— æœ‰ä¸‰)", "ã‚ã‚Šã‹ã›ã‚“", "æœ¬å‰¤ã¯ã•ãŒå¹´ä»¥ä¸Šä¿å­˜ã—ã¦ã‚‚åŠ¹ç›®ã«è®ŠåŒ–ã¯", "æœ¬å‰¤ã¯ä½•ç­‰æœŸä½œå“ã®ãªã„å®‰å…¨ãªå†…æœè–¬ã§ã­", "æœ¬å‰¤ã¯ä»–ã®é†«è—¥ã¨ä½µç”¨ã—ã¦å·®æ”¯ã¸ã‚ã‚Šã¾ã›ã‚“", "ã®å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“", "æœ¬åŠ‘ã¯ã€Œåªä¸€å›åˆ†ã€ã®ã¿ã¦çµæ§‹ã§ã™æˆ˜ç‰ˆ", "ã‚‰å°‚é–€å®¶ã«æœŸç›¸è«‡ä¸‹ã•ã„", "è±¡ã®è¶…ã‚‰ãªã„æ™‚ã¯ä¹¾ç‚ã§ãªã„ã¨æ€ã²ã¾ã™", "ã‚¢ãƒ³ãƒÂ·ãƒ«ãƒ³ã‚²ãƒ³ ã‚’æœç”¨ã—ã¾ã™ã¨å¿…ãšå³ã®", "æœ¬åŠ‘ ã® ç‰¹ é•·", "ã‚‹ã®ã§ã‚ã‚Šã¾ã—ã¦å®‰çœ ã‚ˆã‚Šè¦ºã‚ã‚‹æ™‚ã«ã¯å¾³", "æ­¤ã®ç‰¹æ®Šç¾è±¡ ç™¼æ±—å®‰çœ  ã“ãè–¬åŠ¹ã§æ’²æ»…", "ã¨ã‚‚å¿«ä¸Šã’ã«é•·æ™‚é–“å®‰çœ è‡´ã—ã¾ã™", "ç™¼æ±—ã—å…¶ç™¼æ±—ãŒç›¸ä¹‹æ•™ç”Ÿã‚‹é ƒã‚ˆã‚Šå¿ è€…ã¯ã„", "ã‚¢ãƒ³ãƒÂ·ãƒ«ãƒ³ã‚²ãƒ³ ã‚’æœç”¨ã™ã‚‹ã¨ç™½è¡€çƒã®ã¯", "æ²» ç™‚ ã® éµ å‰‡", "å¦‚ä¸–æ¦®79ç™¼éŸ³å…ƒä¸­å±±", "åƒ¹ å°äººç›¸ é‡‘ åœ‹", "å¤§äººç¿” äº”", "éƒ¨å­¸åŒ–æ‰€é‹¼è£½å±±ä¸­ ç¤¾æœƒå¼æ ¼ å…ƒè³£ç™¼", "ç¤¾æœƒè€…åˆæœ¨å¤§ Â·åº—å•†ç½®ç½ æ„ æŸ)", "ç¤¾æœƒåä»¤éƒ¨å¤ªé•·å·é¾ åœ‹å¤å(", "æœƒé¢ã‚¤ãƒ©ãƒ–ã‚µãƒ«ã‚¿ãƒ“ã‚¹ãƒ› èªª å¤§ãƒ", "åº—ç†ä»£", "ç¤¾æœƒå¼æ ªè—¥è£½äº¬æ± å…ƒé€ è£½", "Image ID: W5DPKC", "www.alamy.com" ] Korean GPT-4V remembers how to extract text again. And it remembers how to get it wrong too! If you look at the output, some of the characters are the same as the image, but some are not. [ "ìˆ ì€ ë‚¨ì— ë§ë‚˜", "ìˆ ì€ ë‚˜ë¡œ ë§ë‚˜" ] SceneXplain gives accurate output, though Iâ€™m not sure where the 50 is coming from: [ "|ì•¼", "ì¢‹ì€ ë‚ ì— ë§Œë‚˜", "ë‹¨ì´ ë°©", "50", "ì¢‹ì€ ë‚˜ë¥¼ ë§Œë‚˜" ] GPT-4V actually does okay with the more complex image. It doesnâ€™t catch all the strings, but itâ€™s less wrong than usual from a quick comparison (note: I donâ€™t read Korean, so Iâ€™m just eyeballing it): [ "ìì—°ê³¼ êµ­ë¯¼ì„ ì—°ê²°í•˜ëŠ”", "ìˆ˜ëª©Â·ì •ì› í”Œë«í¼ ê¸°ê´€", "ìˆ˜ëª©ì›ì˜ ê³ ê°ê°€ì¹˜ë¥¼ ë„˜ì–´", "êµ­ë¯¼ ìƒí™œ ì† ë…¹ìƒ‰ê³µê°„ í™•ì‚°ê³¼", "ì •ì›ì‚°ì—… ìƒíƒœê³„ë¥¼ í™œì„±í™”í•˜ëŠ” ì—­í• ì„", "ì ê·¹ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê² ìŠµë‹ˆë‹¤" ] SceneXplain extracts a lot more: [ "í•œêµ­ìˆ˜ëª©ì›ì •ì›ê´€ë¦¬ì›", "ìì—°ê³¼ êµ­ë¯¼ì„ ì—°ê²°í•˜ëŠ”", "ìˆ˜ëª©ì›Â·ì •ì› í”Œë«í¼ ê¸°ê´€", "ìˆ˜ëª©ì›ì˜ ê³µê°„ì  ê°œë…ì„ ë„˜ì–´", "êµ­ë¯¼ ìƒí™œ ì† ë…¹ìƒ‰ë¬¸í™” í™•ì‚°ê³¼", "ì •ì›ì‚°ì—… ìƒíƒœê³„ë¥¼ í™œì„±í™”í•˜ëŠ” ì—­í• ì„", "ì ê·¹ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê² ìŠµë‹ˆë‹¤", "ì‚°ë¦¼ìƒë¬¼ìì›", "ê³ í’ˆì§ˆ ìˆ˜ëª©ì›", "ES", "ë³´ì „ Â· ë³µì›", "ì„œë¹„ìŠ¤", "ì—­ëŸ‰ ê°•í™”", "ì‚°ë¦¼ìƒë¬¼ìì›", "ì—°êµ¬ Â·í™œìš©", "ì •ì›ë¬¸í™”", "í™œì„±í™”" ] Arabic Since we already covered the Arabic coffee image above, we'll just leave one Arabic here: GPT-4V: [ "Ø§Ù„Ø­ÙŠØ§Ø© Ø·Ø±ÙŠÙ‚ Ø£ÙƒØ«Ø± Ø³Ø±Ø¹Ø©" ] SceneXplain gets a little confused about the 4G logo, seeing it as EAG but is otherwise sound: [ "Ø§Ù„Ø­ÙŠØ§Ø© ØµØ§Ø±Øª Ø£ÙƒØ«Ø± Ø³Ø±Ø¹Ø©", "EAG", "LTE" ] Hindi GPT-4V: [ "à¤¸à¤°à¤¸à¥‹à¤‚ à¤”à¤° à¤†à¤‚à¤µà¤²à¤¾", "à¤•à¤¾ à¤ªà¥‹à¤·à¤£ à¤¬à¤¿à¤¨à¤¾ à¤šà¤¿à¤ªà¤šà¤¿à¤ªà¤¾à¤¹à¤Ÿ", "à¤¸à¤¿à¤°à¥à¤« à¤°à¥", "9 à¤®à¥‡à¤‚", "40ml", "à¤¡à¤¾à¤¬à¤°", "à¤Ÿà¥‹à¤² à¤«à¥à¤°à¥€ 1800-103-1644" ] SceneXplain: [ "à¤¸à¤°à¤¸à¥‹à¤‚ à¤†à¤à¤µà¤²à¤¾", "à¤•à¥‡à¤¶ à¤¤à¥‡à¤²", "à¤¸à¤°à¤¸à¥‹à¤‚ à¤”à¤° à¤†à¤à¤µà¤²à¤¾", "à¤•à¤¾ à¤ªà¥‹à¤·à¤£ à¤¬à¤¿à¤¨à¤¾", "à¤šà¤¿à¤ªà¤šà¤¿à¤ªà¤¾à¤¹à¤Ÿ", "à¤®à¥‡à¤‚", "à¤¨à¤¯à¤¾", "à¤ªà¥ˆà¤•", "â‚¹9/-", "à¤¸à¤°à¤¸à¥‹à¤‚ à¤†à¤à¤µà¤²à¤¾", "40ml", "DABUR CARES: CALL OR WRITE", "+ TOLL FREE 1800-103-1644" ] GPT-4V gave us the error Something went wrong. If this issue persists please contact us through our help center at help.openai.com. After retrying, it gave us: [ "MDH à¤•à¥€ à¤à¤•", "à¤§à¤®à¤¾à¤•à¥‡à¤¦à¤¾à¤° à¤‘à¤«à¤°", "à¤¸à¥à¤µà¤¾à¤¦ à¤•à¥‡ à¤¸à¤¾à¤¥ - à¤¸à¤¾à¤¥ à¤–à¥à¤¶à¤¿à¤¯à¥‹à¤‚ à¤•à¥€ à¤¬à¥Œà¤›à¤¾à¤° !!", "à¤®à¤¸à¤¾à¤²à¤¾ à¤à¤• à¤¸à¥à¤µà¤¾à¤¦à¤¿à¤·à¥à¤Ÿ à¤”à¤° à¤œà¥€à¤µà¤‚à¤¤ à¤‰à¤ªà¤¹à¤¾à¤° !!", "MDH", "à¤®à¤¸à¤¾à¤²à¥‡", "à¤…à¤¸à¤²à¥€ à¤®à¤¸à¤¾à¤²à¥‡ à¤¸à¤š-à¤¸à¤š", "MDH Ltd.", "E-mails: delhi@mdhspices.in, rk@mdhspices.in www.mdhspices.com" ] SceneXplain goes much further, even extracting product names and tiny details like when the company was established: [ "MDH", "à¤•à¥€ à¤à¤•", "à¤§à¤®à¤¾à¤•à¥‡à¤¦à¤¾à¤° à¤‘à¤«à¤°", "à¤¸à¥à¤µà¤¾à¤¦ à¤•à¥‡ à¤¸à¤¾à¤¥ -à¤¸à¤¾à¤¥ à¤–à¥à¤¶à¤¿à¤¯à¥‹à¤‚ à¤•à¥€ à¤¬à¥Œà¤›à¤¾à¤° !", "à¤®à¤¸à¤¾à¤²à¤¾ à¤ªà¥ˆà¤• à¤–à¤°à¥€à¤¦à¥‹ à¤”à¤° à¤œà¥€à¤¤à¥‹ à¤‰à¤ªà¤¹à¤¾à¤° !!", "à¤à¤® à¤¡à¥€ à¤à¤š à¤•à¥€ à¤“à¤° à¤¸à¥‡ à¤…à¤ªà¤¨à¥‡ à¤¸à¤­à¥€ à¤—à¥à¤°à¤¾à¤¹à¤•à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤à¤• à¤§à¤®à¤¾à¤•à¥‡à¤¦à¤¾à¤° à¤‘à¤«à¤°à¥¤", "à¤•à¤¿à¤¸à¥€ à¤­à¥€ à¤¨à¤œà¤¦à¥€à¤•à¥€ à¤µà¤¿à¤•à¥à¤°à¥‡à¤¤à¤¾ à¤¸à¥‡ à¤à¤® à¤¡à¥€ à¤à¤š à¤®à¤¸à¤¾à¤²à¥‹à¤‚ à¤•à¥‡ à¤¨à¥€à¤šà¥‡ à¤›à¤ªà¥‡ à¤ªà¥ˆà¤•à¥à¤¸ à¤®à¥‡à¤‚ à¤¸à¥‡", "à¤•à¥‹à¤ˆ à¤­à¥€ 100 à¤—à¥à¤°à¤¾à¤® à¤µà¤¾à¤²à¤¾ à¤ªà¥ˆà¤• à¤–à¤°à¥€à¤¦à¥‡à¤‚ à¤”à¤° à¤­à¤¾à¤—à¥à¤¯à¤¶à¤¾à¤²à¥€ à¤µà¤¿à¤œà¥‡à¤¤à¤¾ à¤¬à¤¨à¤¨à¥‡ à¤•à¤¾ à¤…à¤µà¤¸à¤°", "à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤•à¤°à¥‡à¤‚, à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ à¤†à¤ª à¤•à¥‡ à¤²à¤¿à¤ à¤—à¤ à¤ªà¥ˆà¤• à¤•à¥‡ à¤«à¥à¤²à¥‡à¤ª à¤®à¥‡à¤‚ à¤‰à¤ªà¤¹à¤¾à¤° à¤•à¤¾ à¤¨à¤¾à¤® à¤¹à¥‹à¥¤", "à¤…à¤¸à¤²à¥€ à¤®à¤¸à¤¾à¤²à¥‡", "à¤®à¤¸à¤¾à¤²à¥‡", "à¤¸à¤š -à¤¸à¤š", "MD H", "SPICES", "MD", "MOH", "Garam", "Pav Bhaji", "masala", "Kitchen", "Shahi Pancer", "King", "Chana", "DEGGI", "MIRCH", "Chunky Chat", "à¤¨à¤¿à¤¯à¤® à¤µ à¤¶à¤°à¥à¤¤à¥‡à¤‚ :- à¤à¤® à¤¡à¥€ à¤à¤š à¤®à¤¸à¤¾à¤²à¥‹à¤‚ à¤•à¥‡ à¤Šà¤ªà¤° à¤¦à¤°à¥à¤¶à¤¾à¤¯à¥‡ à¤—à¤ à¤šà¥à¤¨à¤¿à¤¨à¥à¤¦à¤¾ à¤ªà¥ˆà¤•à¥à¤¸ à¤®à¥‡à¤‚ à¤¸à¥‡ à¤•à¥‹à¤ˆ à¤­à¥€ 100 à¤—à¥à¤°à¤¾à¤® à¤µà¤¾à¤²à¤¾", "à¤ªà¥ˆà¤• à¤–à¤°à¥€à¤¦à¥‡à¤‚, à¤‰à¤¸à¤•à¤¾ à¤«à¥à¤²à¥‡à¤ª à¤–à¥‹à¤²à¥‡à¤‚, à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤‰à¤¸ à¤«à¥à¤²à¥‡à¤ª à¤•à¥‡ à¤¨à¥€à¤šà¥‡ à¤‰à¤ªà¤¹à¤¾à¤° à¤•à¤¾ à¤¨à¤¾à¤® à¤›à¤ªà¤¾ à¤¹à¥‹à¥¤ à¤‰à¤¸", "à¤‰à¤ªà¤¹à¤¾à¤° à¤•à¥‹ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤­à¤¾à¤—à¥à¤¯à¤¶à¤¾à¤²à¥€ à¤µà¤¿à¤œà¥‡à¤¤à¤¾ à¤¬à¤¨à¥‡à¤‚à¥¤ â€¢ à¤‡à¤¸ à¤¸à¥à¤•à¥€à¤® à¤•à¤¾ à¤¨à¤•à¤¦ à¤²à¤¾à¤­ à¤•à¥‹à¤ˆ à¤­à¥€ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆ", "à¤²à¤¿à¤ à¤¦à¤¿à¤²à¥à¤²à¥€ à¤®à¥‡à¤‚ à¤¹à¥€ à¤²à¤¾à¤—à¥‚ à¤¹à¥ˆ â€¢ à¤¸à¥à¤•à¥€à¤® à¤¸à¥à¤Ÿà¥‰à¤• à¤°à¤¹à¤¨à¥‡ à¤¤à¤• à¤¸à¥€à¤®à¤¿à¤¤ à¤¹à¥ˆ â€¢ à¤¯à¤¹ à¤¸à¥à¤•à¥€à¤® à¤à¤® à¤¡à¥€ à¤à¤š à¤•à¥‡ à¤¸à¥à¤ªà¤°", "à¤¸à¥à¤Ÿà¥‰à¤•à¤¿à¤¸à¥à¤Ÿ, à¤¸à¥à¤Ÿà¥‰à¤•à¤¿à¤¸à¥à¤Ÿ à¤”à¤° à¤•à¤°à¥à¤®à¤šà¤¾à¤°à¤¿à¤¯à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆ Â· à¤•à¤‚à¤ªà¤¨à¥€ à¤•à¤¾ à¤¨à¤¿à¤°à¥à¤£à¤¯ à¤…à¤‚à¤¤à¤¿à¤® à¤”à¤° à¤¸à¤°à¥à¤µà¤®à¤¾à¤¨à¥à¤¯ à¤¹à¥‹à¤—à¤¾", "à¤”à¤° à¤¸à¤­à¥€ à¤µà¤¿à¤µà¤¾à¤¦ à¤¦à¤¿à¤²à¥à¤²à¥€ à¤•à¥‡ à¤¨à¥à¤¯à¤¾à¤¯ à¤•à¥à¤·à¥‡à¤¤à¥à¤° à¤•à¥‡ à¤…à¤§à¥€à¤¨ à¤¹à¥‹à¤‚à¤—à¥‡à¥¤ â€¢ à¤…à¤¨à¥à¤¯ à¤¶à¤°à¥à¤¤à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤ªà¥ˆà¤• à¤¦à¥‡à¤–à¥‡à¤‚à¥¤", "à¤®à¤¹à¤¾à¤¶à¤¿à¤¯à¤¾à¤ à¤¦à¥€ à¤¹à¤Ÿà¥à¤Ÿà¥€ (à¤ªà¥à¤°à¤¾à¥¦) à¤²à¤¿à¤®à¤¿à¤Ÿà¥‡à¤¡", "9/44, à¤•à¥€à¤°à¥à¤¤à¤¿ à¤¨à¤—à¤°, à¤¨à¤ˆ à¤¦à¤¿à¤²à¥à¤²à¥€ - 110015 à¤«à¥‹à¤¨ à¤¨à¤‚à¥¦ 011-41425106 - 07-08", "ESTD. 1919", "E-mails : delhi@mdhspices.in, rk@mdhspices.in www.mdhspices.com" ] Conclusion At the end of the day, SceneXplain clearly excels in multilingual OCR compared to GPT-4V. To recap: API limits: GPT-4V: 100 requests per day SceneXplain: 5,000 credits per month on the MAX plan. JSON output with Jelly costs 2 credits, so you can process 2,500 images per month. Reliability: GPT-4V: Is often like â€œIâ€™m sorry Dave, Iâ€™m afraid I canâ€™t do that.â€ SceneXplain: Doggedly reliable. Gets the job done. Accuracy: GPT-4V: hallucinates strings it believes are plausible from a given image, rather than reading the actual text. SceneXplain: Extracts the correct text. Consistency: GPT-4V doesnâ€™t just hallucinate. It hallucinates different things each time. SceneXplain gives much more consistent output. Completeness: GPT-4V: Often doesnâ€™t always extract all the strings in the image. SceneXplain: Extracts more strings. Cost: GPT-4V: $0.025 per image. SceneXplain: As low as 0.02 USD per image on the MAX plan if you enable JSON output. As low as 0.01 USD without JSON output. When you take all those factors into account, it's clear that SceneXplain is the obvious choice. Get started with SceneXplain Ready to explore multilingual OCR? Sign up for a free account with SceneXplain and start converting images to text easily. It's a practical, user-friendly way to manage digital documents. Get started now and see the difference SceneXplain can make! SceneXplain - Leading AI Solution for Image Captions and Video Summaries Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today. SceneXplain Categories: Tech blog rss_feed Top-5 similar articles play_arrow GET TOP-5 Select reranker Read more

August 26, 2024 â€¢ 13 minutes read The What and Whyof Text-Image Modality Gap in CLIP Models You can't just use a CLIP model to retrieve text and images and sort the results by score. Why? Because of the modality gap. What is it, and where does it come from? August 22, 2024 â€¢ 8 minutes read

Late Chunking in Long-Context Embedding Models Chunking long documents while preserving contextual information is challenging. We introduce the "Late Chunking" that leverages long-context embedding models to generate contextual chunk embeddings for better retrieval applications.

July 31, 2024 â€¢ 17 minutes read Rephrased Labels Improve Zero-Shot Text Classification by 30%

When using embedding models for zero-shot classification, rephrasing the class label to "This is seriously about 'LABEL'" gives higher accuracy vs. using LABEL alone. But how, and why? OFFICES location_on Berlin, Germany (HQ) PrinzessinnenstraÃŸe 19-20, 10969 Berlin, Germany GeschÃ¤ftsanschrift: Leipzigerstr. 96, 10117 Berlin, Germany location_on Beijing, China Level 5, Building 6, No.48 Haidian West St. Beijing Haidian, China location_on Shenzhen, China 402, Floor 4, Fu'an Technology Building, Shenzhen Nanshan, China SEARCH FOUNDATION Embeddings Reranker Reader Segmenter Get Jina AI API key API Status COMPANY About us Contact sales Newsroom Intern program Join us open_in_new Download logo open_in_new TERMS Terms & Conditions Privacy Manage Cookies email language English science Jina AI GmbH Â© 2020-2024.

## SUMMARY_CHUNKING

#### 8 chunk(s)

SceneXplain But just because you take an early lead doesn't mean you win the race. We've got to show how we're better, right? Well, we've already done that in this post: SceneXplain vs. GPT-4 Vision: The Best Image Captioning Tool in 2023? Discover the future of visual comprehension with SceneXplain, the leading image captioning tool of 2023. Dive deep into its transformative features, real-world applications, and see how it stands tall against GPT-4 Vision. SceneXplain But in this post, we'll go one further and show you how we excel at OCR for multilingual texts. In short, SceneXplain is more accurate, reliable, consistent, complete, and cost-effective than GPT-4V. Also, it doesnâ€™t have any pesky daily request limits. You might think GPT-4V is pretty decent if youâ€™re a user of Latin-character languages. Since youâ€™re reading this (I assume in English), that means you! Indeed, GPT-4V does okay when it comes to recognizing English, French, Spanish, and so on, as you can see from these blog posts and studies:

SceneXplain But in this post, we'll go one furtherand show you how we excel at OCR for multilingual texts. In short, SceneXplain is more accurate, reliable, consistent, complete, and cost-effective than GPT-4V. Also, it doesnâ€™t have any pesky daily request limits. You might think GPT-4V is pretty decent if youâ€™re a user of Latin-character languages. Since youâ€™re reading this (I assume in English), that means you! Indeed, GPT-4V does okay when it comes to recognizing English, French, Spanish, and so on, as you can see from these blog posts and studies: First Impressions with GPT-4V(ision) Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation But, as mentioned in the study: There is a substantial accuracy disparity between the recognition of English and Chinese text. As shown in Table 1, the performance of English text recognition is commendable. Conversely, the accuracy of Chinese text recognition is zero (ReCTS). We speculate that this may be due to the lack of Chinese scene text images as training data in GPT-4V. Bad news - it's not just Chinese where GPT-4V falls down. It consistently fails on non-Latin texts. That's where SceneXplain comes in. We go way beyond just recognizing Latin characters - we can do Chinese, Japanese, Korean, Arabic, Hindi, and potentially more (at this point we ran out of languages that people speak (or recognize) at Jina AI.) TL;DR: How does GPT-4V fail compared to SceneXplain? In short: API limits: 100 requests a day is the new â€œ640K ought to be enough for anybodyâ€. SceneXplain has a range of plans to suit your needs. Reliability: Sometimes, GPT-4V will follow a command to extract the text strings. But if you ask again in exactly the same way, it may say it canâ€™t do it. SceneXplain gets it done right every time. Accuracy: GPT-4V will often extract strings that are plausible for a given image, but not the actual strings. For example, if you give it a Chinese menu, it will hallucinate Chinese dish names. SceneXplain pretty much cured hallucinations in its Jelly algorithm. Consistency: GPT-4V doesnâ€™t just hallucinate. It hallucinates different things each time.

SceneXplain gives accurate output, though Iâ€™m not sure where the 50 is coming from: [ "|ì•¼", "ì¢‹ì€ ë‚ ì— ë§Œë‚˜", "ë‹¨ì´ ë°©", "50", "ì¢‹ì€ ë‚˜ë¥¼ ë§Œë‚˜" ]

GPT-4V actually does okay with the more complex image. It doesnâ€™t catch all the strings, but itâ€™s less wrong than usual from a quick comparison (note: I donâ€™t read Korean, so Iâ€™m just eyeballing it): [ "ìì—°ê³¼ êµ­ë¯¼ì„ ì—°ê²°í•˜ëŠ”", "ìˆ˜ëª©Â·ì •ì› í”Œë«í¼ ê¸°ê´€", "ìˆ˜ëª©ì›ì˜ ê³ ê°ê°€ì¹˜ë¥¼ ë„˜ì–´", "êµ­ë¯¼ ìƒí™œ ì† ë…¹ìƒ‰ê³µê°„ í™•ì‚°ê³¼", "ì •ì›ì‚°ì—… ìƒíƒœê³„ë¥¼ í™œì„±í™”í•˜ëŠ” ì—­í• ì„", "ì ê·¹ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê² ìŠµë‹ˆë‹¤" ] SceneXplain extracts a lot more: [ "í•œêµ­ìˆ˜ëª©ì›ì •ì›ê´€ë¦¬ì›", "ìì—°ê³¼ êµ­ë¯¼ì„ ì—°ê²°í•˜ëŠ”", "ìˆ˜ëª©ì›Â·ì •ì› í”Œë«í¼ ê¸°ê´€", "ìˆ˜ëª©ì›ì˜ ê³µê°„ì  ê°œë…ì„ ë„˜ì–´", "êµ­ë¯¼ ìƒí™œ ì† ë…¹ìƒ‰ë¬¸í™” í™•ì‚°ê³¼", "ì •ì›ì‚°ì—… ìƒíƒœê³„ë¥¼ í™œì„±í™”í•˜ëŠ” ì—­í• ì„", "ì ê·¹ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê² ìŠµë‹ˆë‹¤", "ì‚°ë¦¼ìƒë¬¼ìì›", "ê³ í’ˆì§ˆ ìˆ˜ëª©ì›", "ES", "ë³´ì „ Â· ë³µì›", "ì„œë¹„ìŠ¤", "ì—­ëŸ‰ ê°•í™”", "ì‚°ë¦¼ìƒë¬¼ìì›", "ì—°êµ¬ Â·í™œìš©", "ì •ì›ë¬¸í™”", "í™œì„±í™”" ] Arabic Since we already covered the Arabic coffee image above, we'll just leave one Arabic here: GPT-4V: [ "Ø§Ù„Ø­ÙŠØ§Ø© Ø·Ø±ÙŠÙ‚ Ø£ÙƒØ«Ø± Ø³Ø±Ø¹Ø©" ] SceneXplain gets a little confused about the 4G logo, seeing it as EAG but is otherwise sound: [ "Ø§Ù„Ø­ÙŠØ§Ø© ØµØ§Ø±Øª Ø£ÙƒØ«Ø± Ø³Ø±Ø¹Ø©", "EAG", "LTE" ] Hindi GPT-4V: [ "à¤¸à¤°à¤¸à¥‹à¤‚ à¤”à¤° à¤†à¤‚à¤µà¤²à¤¾", "à¤•à¤¾ à¤ªà¥‹à¤·à¤£ à¤¬à¤¿à¤¨à¤¾ à¤šà¤¿à¤ªà¤šà¤¿à¤ªà¤¾à¤¹à¤Ÿ", "à¤¸à¤¿à¤°à¥à¤« à¤°à¥", "9 à¤®à¥‡à¤‚", "40ml", "à¤¡à¤¾à¤¬à¤°", "à¤Ÿà¥‹à¤² à¤«à¥à¤°à¥€ 1800-103-1644" ] SceneXplain: [ "à¤¸à¤°à¤¸à¥‹à¤‚ à¤†à¤à¤µà¤²à¤¾", "à¤•à¥‡à¤¶ à¤¤à¥‡à¤²", "à¤¸à¤°à¤¸à¥‹à¤‚ à¤”à¤° à¤†à¤à¤µà¤²à¤¾", "à¤•à¤¾ à¤ªà¥‹à¤·à¤£ à¤¬à¤¿à¤¨à¤¾", "à¤šà¤¿à¤ªà¤šà¤¿à¤ªà¤¾à¤¹à¤Ÿ", "à¤®à¥‡à¤‚", "à¤¨à¤¯à¤¾", "à¤ªà¥ˆà¤•", "â‚¹9/-", "à¤¸à¤°à¤¸à¥‹à¤‚ à¤†à¤à¤µà¤²à¤¾", "40ml", "DABUR CARES: CALL OR WRITE", "+ TOLL FREE 1800-103-1644" ] GPT-4V gave us the error Something went wrong. If this issue persists please contact us through our help center at help.openai.com. After retrying, it gave us: [ "MDH à¤•à¥€ à¤à¤•", "à¤§à¤®à¤¾à¤•à¥‡à¤¦à¤¾à¤° à¤‘à¤«à¤°", "à¤¸à¥à¤µà¤¾à¤¦ à¤•à¥‡ à¤¸à¤¾à¤¥ - à¤¸à¤¾à¤¥ à¤–à¥à¤¶à¤¿à¤¯à¥‹à¤‚ à¤•à¥€ à¤¬à¥Œà¤›à¤¾à¤° !!", "à¤®à¤¸à¤¾à¤²à¤¾ à¤à¤• à¤¸à¥à¤µà¤¾à¤¦à¤¿à¤·à¥à¤Ÿ à¤”à¤° à¤œà¥€à¤µà¤‚à¤¤ à¤‰à¤ªà¤¹à¤¾à¤° !!", "MDH", "à¤®à¤¸à¤¾à¤²à¥‡", "à¤…à¤¸à¤²à¥€ à¤®à¤¸à¤¾à¤²à¥‡ à¤¸à¤š-à¤¸à¤š", "MDH Ltd.", "E-mails: delhi@mdhspices.in, rk@mdhspices.in www.mdhspices.com" ]

SceneXplain goes much further, even extracting product names and tiny details like when the company was established: [ "MDH", "à¤•à¥€ à¤à¤•", "à¤§à¤®à¤¾à¤•à¥‡à¤¦à¤¾à¤° à¤‘à¤«à¤°", "à¤¸à¥à¤µà¤¾à¤¦ à¤•à¥‡ à¤¸à¤¾à¤¥ -à¤¸à¤¾à¤¥ à¤–à¥à¤¶à¤¿à¤¯à¥‹à¤‚ à¤•à¥€ à¤¬à¥Œà¤›à¤¾à¤° !", "à¤®à¤¸à¤¾à¤²à¤¾ à¤ªà¥ˆà¤• à¤–à¤°à¥€à¤¦à¥‹ à¤”à¤° à¤œà¥€à¤¤à¥‹ à¤‰à¤ªà¤¹à¤¾à¤° !!", "à¤à¤® à¤¡à¥€ à¤à¤š à¤•à¥€ à¤“à¤° à¤¸à¥‡ à¤…à¤ªà¤¨à¥‡ à¤¸à¤­à¥€ à¤—à¥à¤°à¤¾à¤¹à¤•à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤à¤• à¤§à¤®à¤¾à¤•à¥‡à¤¦à¤¾à¤° à¤‘à¤«à¤°à¥¤", "à¤•à¤¿à¤¸à¥€ à¤­à¥€ à¤¨à¤œà¤¦à¥€à¤•à¥€ à¤µà¤¿à¤•à¥à¤°à¥‡à¤¤à¤¾ à¤¸à¥‡ à¤à¤® à¤¡à¥€ à¤à¤š à¤®à¤¸à¤¾à¤²à¥‹à¤‚ à¤•à¥‡ à¤¨à¥€à¤šà¥‡ à¤›à¤ªà¥‡ à¤ªà¥ˆà¤•à¥à¤¸ à¤®à¥‡à¤‚ à¤¸à¥‡", "à¤•à¥‹à¤ˆ à¤­à¥€ 100 à¤—à¥à¤°à¤¾à¤® à¤µà¤¾à¤²à¤¾ à¤ªà¥ˆà¤• à¤–à¤°à¥€à¤¦à¥‡à¤‚ à¤”à¤° à¤­à¤¾à¤—à¥à¤¯à¤¶à¤¾à¤²à¥€ à¤µà¤¿à¤œà¥‡à¤¤à¤¾ à¤¬à¤¨à¤¨à¥‡ à¤•à¤¾ à¤…à¤µà¤¸à¤°", "à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤•à¤°à¥‡à¤‚, à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ à¤†à¤ª à¤•à¥‡ à¤²à¤¿à¤ à¤—à¤ à¤ªà¥ˆà¤• à¤•à¥‡ à¤«à¥à¤²à¥‡à¤ª à¤®à¥‡à¤‚ à¤‰à¤ªà¤¹à¤¾à¤° à¤•à¤¾ à¤¨à¤¾à¤® à¤¹à¥‹à¥¤", "à¤…à¤¸à¤²à¥€ à¤®à¤¸à¤¾à¤²à¥‡", "à¤®à¤¸à¤¾à¤²à¥‡", "à¤¸à¤š -à¤¸à¤š", "MD H", "SPICES", "MD", "MOH", "Garam", "Pav Bhaji", "masala", "Kitchen", "Shahi Pancer", "King", "Chana", "DEGGI", "MIRCH", "Chunky Chat", "à¤¨à¤¿à¤¯à¤® à¤µ à¤¶à¤°à¥à¤¤à¥‡à¤‚ :- à¤à¤® à¤¡à¥€ à¤à¤š à¤®à¤¸à¤¾à¤²à¥‹à¤‚ à¤•à¥‡ à¤Šà¤ªà¤° à¤¦à¤°à¥à¤¶à¤¾à¤¯à¥‡ à¤—à¤ à¤šà¥à¤¨à¤¿à¤¨à¥à¤¦à¤¾ à¤ªà¥ˆà¤•à¥à¤¸ à¤®à¥‡à¤‚ à¤¸à¥‡ à¤•à¥‹à¤ˆ à¤­à¥€ 100 à¤—à¥à¤°à¤¾à¤® à¤µà¤¾à¤²à¤¾", "à¤ªà¥ˆà¤• à¤–à¤°à¥€à¤¦à¥‡à¤‚, à¤‰à¤¸à¤•à¤¾ à¤«à¥à¤²à¥‡à¤ª à¤–à¥‹à¤²à¥‡à¤‚, à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤‰à¤¸ à¤«à¥à¤²à¥‡à¤ª à¤•à¥‡ à¤¨à¥€à¤šà¥‡ à¤‰à¤ªà¤¹à¤¾à¤° à¤•à¤¾ à¤¨à¤¾à¤® à¤›à¤ªà¤¾ à¤¹à¥‹à¥¤ à¤‰à¤¸", "à¤‰à¤ªà¤¹à¤¾à¤° à¤•à¥‹ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤­à¤¾à¤—à¥à¤¯à¤¶à¤¾à¤²à¥€ à¤µà¤¿à¤œà¥‡à¤¤à¤¾ à¤¬à¤¨à¥‡à¤‚à¥¤ â€¢ à¤‡à¤¸ à¤¸à¥à¤•à¥€à¤® à¤•à¤¾ à¤¨à¤•à¤¦ à¤²à¤¾à¤­ à¤•à¥‹à¤ˆ à¤­à¥€ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆ", "à¤²à¤¿à¤ à¤¦à¤¿à¤²à¥à¤²à¥€ à¤®à¥‡à¤‚ à¤¹à¥€ à¤²à¤¾à¤—à¥‚ à¤¹à¥ˆ â€¢ à¤¸à¥à¤•à¥€à¤® à¤¸à¥à¤Ÿà¥‰à¤• à¤°à¤¹à¤¨à¥‡ à¤¤à¤• à¤¸à¥€à¤®à¤¿à¤¤ à¤¹à¥ˆ â€¢ à¤¯à¤¹ à¤¸à¥à¤•à¥€à¤® à¤à¤® à¤¡à¥€ à¤à¤š à¤•à¥‡ à¤¸à¥à¤ªà¤°", "à¤¸à¥à¤Ÿà¥‰à¤•à¤¿à¤¸à¥à¤Ÿ, à¤¸à¥à¤Ÿà¥‰à¤•à¤¿à¤¸à¥à¤Ÿ à¤”à¤° à¤•à¤°à¥à¤®à¤šà¤¾à¤°à¤¿à¤¯à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆ Â· à¤•à¤‚à¤ªà¤¨à¥€ à¤•à¤¾ à¤¨à¤¿à¤°à¥à¤£à¤¯ à¤…à¤‚à¤¤à¤¿à¤® à¤”à¤° à¤¸à¤°à¥à¤µà¤®à¤¾à¤¨à¥à¤¯ à¤¹à¥‹à¤—à¤¾", "à¤”à¤° à¤¸à¤­à¥€ à¤µà¤¿à¤µà¤¾à¤¦ à¤¦à¤¿à¤²à¥à¤²à¥€ à¤•à¥‡ à¤¨à¥à¤¯à¤¾à¤¯ à¤•à¥à¤·à¥‡à¤¤à¥à¤° à¤•à¥‡ à¤…à¤§à¥€à¤¨ à¤¹à¥‹à¤‚à¤—à¥‡à¥¤ â€¢ à¤…à¤¨à¥à¤¯ à¤¶à¤°à¥à¤¤à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤ªà¥ˆà¤• à¤¦à¥‡à¤–à¥‡à¤‚à¥¤", "à¤®à¤¹à¤¾à¤¶à¤¿à¤¯à¤¾à¤ à¤¦à¥€ à¤¹à¤Ÿà¥à¤Ÿà¥€ (à¤ªà¥à¤°à¤¾à¥¦) à¤²à¤¿à¤®à¤¿à¤Ÿà¥‡à¤¡", "9/44, à¤•à¥€à¤°à¥à¤¤à¤¿ à¤¨à¤—à¤°, à¤¨à¤ˆ à¤¦à¤¿à¤²à¥à¤²à¥€ - 110015 à¤«à¥‹à¤¨ à¤¨à¤‚à¥¦ 011-41425106 - 07-08", "ESTD. 1919", "E-mails : delhi@mdhspices.in, rk@mdhspices.in www.mdhspices.com" ]

Conclusion At the end of the day, SceneXplain clearly excels in multilingual OCR compared to GPT-4V. To recap: API limits: GPT-4V: 100 requests per day SceneXplain: 5,000 credits per month on the MAX plan. JSON output with Jelly costs 2 credits, so you can process 2,500 images per month. Reliability: GPT-4V: Is often like â€œIâ€™m sorry Dave, Iâ€™m afraid I canâ€™t do that.â€ SceneXplain: Doggedly reliable. Gets the job done. Accuracy: GPT-4V: hallucinates strings it believes are plausible from a given image, rather than reading the actual text. SceneXplain: Extracts the correct text. Consistency: GPT-4V doesnâ€™t just hallucinate. It hallucinates different things each time. SceneXplain gives much more consistent output. Completeness: GPT-4V: Often doesnâ€™t always extract all the strings in the image. SceneXplain: Extracts more strings. Cost: GPT-4V: $0.025 per image. SceneXplain: As low as 0.02 USD per image on the MAX plan if you enable JSON output. As low as 0.01 USD without JSON output. When you take all those factors into account, it's clear that SceneXplain is the obvious choice.

Get started with SceneXplain Ready to explore multilingual OCR? Sign up for a free account with SceneXplain and start converting images to text easily. It's a practical, user-friendly way to manage digital documents. Get started now and see the difference SceneXplain can make! SceneXplain - Leading AI Solution for Image Captions and Video Summaries Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today. SceneXplain Categories: Tech blog rss_feed Top-5 similar articles play_arrow GET TOP-5 Select reranker Read more August 26, 2024 â€¢ 13 minutes read The What and Why of Text-Image Modality Gap in CLIP Models You can't just use a CLIP model to retrieve text and images and sort the results by score. Why? Because of the modality gap. What is it, and where does it come from? August 22, 2024 â€¢ 8 minutes read Late Chunking in Long-Context Embedding Models Chunking long documents while preserving contextual information is challenging. We introduce the "Late Chunking" that leverages long-context embedding models to generate contextual chunk embeddings for better retrieval applications. July 31, 2024 â€¢ 17 minutes read Rephrased Labels Improve Zero-Shot Text Classification by 30% When using embedding models for zero-shot classification, rephrasing the class label to "This is seriously about 'LABEL'" gives higher accuracy vs. using LABEL alone. But how, and why?

OFFICES location_on Berlin, Germany (HQ) PrinzessinnenstraÃŸe 19-20, 10969 Berlin, Germany GeschÃ¤ftsanschrift: Leipzigerstr. 96, 10117 Berlin, Germany location_on Beijing, China Level 5, Building 6, No.48 Haidian West St. Beijing Haidian, China location_on Shenzhen, China 402, Floor 4, Fu'an Technology Building, Shenzhen Nanshan, China SEARCH FOUNDATION Embeddings Reranker Reader Segmenter Get Jina AI API key API Status COMPANY About us Contact sales Newsroom Intern program Join us open_in_new Download logo open_in_new TERMS Terms & Conditions Privacy Manage Cookies email language English science Jina AI GmbH Â© 2020-2024.

## JINA-SEGMENTER-API

#### 331 chunk(s)

search


notifications


NEWS


PRODUCTS


COMPANY


Tech blog


November 16, 2023


SceneXplain's OCR beats GPT-4V hands down: Chinese, Japanese, Korean, Arabic, Hindi, and more!


SceneXplain beats GPT-4V by going beyond English, offering multilingual OCR for Chinese, Japanese, Korean, Arabic, Hindi and more


Alex C-G â€¢ 13 minutes read



There's been a lot of noise recently about the release of GPT-4V - a model that can analyze the content of images and summarize that into human language. Sound familiar? It may, because SceneXplain has been doing just that for months already!



SceneXplain - Leading AI Solution for Image Captions and Video Summaries


Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.


SceneXplain



But just because you take an early lead doesn't mean you win the race. We've got to show how we're better, right? Well, we've already done that in this post:



SceneXplain vs. GPT-4 Vision: The Best Image Captioning Tool in 2023?


Discover the future of visual comprehension with SceneXplain, the leading image captioning tool of 2023. Dive deep into its transformative features, real-world applications, and see how it stands tall against GPT-4 Vision.


SceneXplain



But in this post, we'll go one further and show you how we excel at OCR for multilingual texts. In short, SceneXplain is more accurate, reliable, consistent, complete, and cost-effective than GPT-4V. Also, it doesnâ€™t have any pesky daily request limits.



You might think GPT-4V is pretty decent if youâ€™re a user of Latin-character languages. Since youâ€™re reading this (I assume in English), that means you!



Indeed, GPT-4V does okay when it comes to recognizing English, French, Spanish, and so on, as you can see from these blog posts and studies:



First Impressions with GPT-4V(ision)


Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation



But, as mentioned in the study:



There is a substantial accuracy disparity between the recognition of English and Chinese text. As shown in Table 1, the performance of English text recognition is commendable. Conversely, the accuracy of Chinese text recognition is zero (ReCTS). We speculate that this may be due to the lack of Chinese scene text images as training data in GPT-4V.



Bad news - it's not just Chinese where GPT-4V falls down. It consistently fails on non-Latin texts.



That's where SceneXplain comes in. We go way beyond just recognizing Latin characters - we can do Chinese, Japanese, Korean, Arabic, Hindi, and potentially more (at this point we ran out of languages that people speak (or recognize) at Jina AI.)



TL;DR: How does GPT-4V fail compared to SceneXplain?



In short:



API limits: 100 requests a day is the new â€œ640K ought to be enough for anybodyâ€. SceneXplain has a range of plans to suit your needs.


Reliability: Sometimes, GPT-4V will follow a command to extract the text strings. But if you ask again in exactly the same way, it may say it canâ€™t do it. SceneXplain gets it done right every time.


Accuracy: GPT-4V will often extract strings that are plausible for a given image, but not the actual strings. For example, if you give it a Chinese menu, it will hallucinate Chinese dish names. SceneXplain pretty much cured hallucinations in its Jelly algorithm.


Consistency: GPT-4V doesnâ€™t just hallucinate. It hallucinates different things each time. SceneXplain is generally consistent.


Completeness: GPT-4V doesnâ€™t always extract all the strings in the image. SceneXplain is more comprehensive.


Cost: SceneXplain is up to 50% more affordable.



Letâ€™s dive into each point in more detail.



API limits



Good luck with the GPT-4V API! In our extensive testing (even just to write this blog post), we rapidly and repeatedly hit request limits. An organization is limited to 100 requests per day, along with strict throttling. We would've put this post out earlier if we hadn't easily overshot that multiple times.



Hello RateLimitError, my old friend.


Reliability



If you do try to extract strings via the web interface, results can be unpredictable. It can handle English well enough:



We can extract the strings as JSON by using the prompt:



Extract the text strings from this image and populate a JSON that follows this schema:



{


"type": "object",
  

"properties": {
    

"text_strings": {
      

"type": "array",
      

"description": "Every text string contained in the image. Consider all languages"
    }
  }
}



Return just the output JSON. Do not put it in a code block




Note how specific we had to be in this prompt, explaining that we just wanted raw JSON back, not including comments like â€œHere is your JSONâ€ and then a back-ticked code block. There is still no guarantee that such a method is watertight. It may sometimes return extraneous content.



It returns this output:



{


"text_strings": [


"YOU CAN'T RUN",


"FROM US FOREVER",


"m"
]
}




I wouldâ€™ve preferred some prettier, indented output. But it looks like valid JSON and accurate output. For reference, SceneXplain returns:



{


"text_strings": [
    

"YOU CAN'T RUN",
    

"FROM US FOREVER",
    

"M"
  ]
}



Ah, those sweet sweet indents



Now letâ€™s try asking in natural language about a Chinese ad:



If we use the prompt: Extract the text strings from this image, GPT-4V gives us:



It seems there was an error while attempting to extract the text from the image using OCR. The error message indicates a problem with the Tesseract OCR data files for the Chinese simplified language. The necessary language files may not be installed in the environment where the OCR operation was attempted.



Given this issue, I'm unable to extract the text from the image as requested. If there's anything else you would like me to assist you with, please let me know!




Thatâ€™sâ€¦not useful. However, asking it to extract it as JSON (by using the above prompt with schema) works:



{


"text_strings": [


"éº¦å½“åŠ³",


"ä¸€å‘¨ä¸ƒå¤©",


"æ—©æ™¨å››ç‚¹",


"è‡³",


"17:00 æ—¶æ®µ",


"å‘¨æœ«æ—©é¤ å»¶æ—¶",


"æ›´å¤šæƒŠå–œ",


"å°½åœ¨éº¦å½“åŠ³APP"
]
}




Why can it do the job if we specify we want JSON output, but it just shrugs helplessly if we ask in plain English? Who knows.



Accuracy



Take another look at that JSON output. Then, take a look at the ad.



At first glance, the JSON looks decent. Itâ€™s got the 17:00 and other details. But take a closer look. None of those text strings appear in the image. They look like the kind of strings that could occur in a Chinese McDonald's ad, but none of them are actually in that particular ad.



This can be even more dangerous than being wrong - enough people are already worried about GPT mansplaining - i.e., being confident but wrong. We see the same story when it comes to non-Latin text too.



Also, why does it suddenly think it can extract the text when it stated before that it couldnâ€™t?



For reference, SceneXplain returns:



{


"text_strings": [
    

"éº¦éº¦å¤œå¸‚",
    

"å‡ºç¥å¤å‘³é¸¡æ¶",
    

"é’èŠ±æ¤’æ‹Œé¸¡",
    

"é’èŠ±æ¤’é…¸è¾£é¸¡",
    

"é’èŠ±æ¤’åŠé¸¡",
    

"éº¦éº¦å¤œå¸‚å°å ¡",
    

"æš—ç³–å¸¦åŠ²å ¡",
    

"å”¯é†‡å¸¦åŠ²å ¡",
    

"è±è±çœŸé¦™å ¡",
    

"17:00è¥å¤œ",
    

"å“‡è—•å¸¦åŠ²å ¡"
  ]
}




If you compare the strings, youâ€™ll see SceneXplain is much more accurate and hallucinates less.



Consistency



Coming back to our old friend, the Chinese McDonaldâ€™s ad, what if we start a new chat session and re-submit the exact same image and prompt? Here's what we get:



{


"type": "object",


"properties": {


"text_strings": [


"éº¦è¾£é¸¡ç¿…",


"å ¡å ¡åŒæ‹¼ ç­‰ä½ æ¥æ’©",


"ä¸€äººé£Ÿå¥½ç¦åˆ©",


"17:00å‰æ¥",


"éº¦å½“åŠ³",


"å°Šäº«è½¯æ¬§åŒ…",


"å‘¨è¾¹å¥½è´§ ç­‰ä½ ",


"å°Šäº«è½¯æ¬§åŒ…"
]
}
}




Well dang. Not only is the text entirely different, but itâ€™s also sent us JSON in a completely different format, something like the schema we sent, rather than the output format that the schema defines.



Completeness



If we try to extract text from this Arabic coffee ad, GPT-4V gives us far fewer strings than SceneXplain does:



GPT-4V:


[


"Ù…Ø¹ØªÙˆÙ‚ MAATOUK",


"1960",


"Ù‚Ù‡ÙˆØ© Ø¹Ø±Ø¨ÙŠØ©",


"Ø­Ø¨ÙŠØ¨Ø© Ø§Ù„Ø£ØµÙˆÙ„",


"ØªÙ‚Ø¯Ù…Ø© Ø¹Ø±Ø¨ÙŠØ©",


"Ø£ØµØ§Ù„Ø© Ø§Ù…ØªØ¯Øª ÙˆØ³Ø·ÙˆÙ„Ø© Ø§Ù„Ù†Ø¶Ø¬"
]



SceneXplain:


[


    "Ù…Ø¹ØªÙˆÙ‚",
    "MAATOUK",
    "1 9 6 0",
    "Ù‚Ù‡ÙˆØ© Ø¹Ø±Ø¨ÙŠØ©",
    "Ø­Ø³Ø¨ Ø§Ù„Ø£ØµÙˆÙ„",
    "Ù‚Ù‡ÙˆØ© Ø¹Ø±Ø¨ÙŠØ©",
    "Ù…Ø¹ØªÙˆÙ‚",
    "mAATOUK",
    "ØªØ­Ù…ÙŠØµ ØºØ§Ù…Ù‚",
    "Ø¨Ù† Ù…Ø·Ø­ÙˆÙ† Ù¡Ù Ù Ùª Ø£Ø±Ø§Ø¨ÙŠÙƒØ§",
    "Ø£ØµØ§Ù„Ø© Ø§Ù„Ù…Ø°Ø§Ù‚ ÙˆØ³Ù‡ÙˆÙ„Ø© Ø§Ù„ØªØ­Ø¶ÙŠØ±",
    "Ù„Ø·Ø§Ù„Ù…Ø§ Ø§Ø´ØªÙ‡Ø± Ø§Ù„Ø¹Ø±Ø¨ Ø¹Ø¨Ø± Ù…Ø§Ø¶ÙŠ Ø§Ù„Ø²Ù…Ø§Ù† Ø¨ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù‚Ù‡ÙˆØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙŠ Ù…Ù†Ø§Ø²Ù„Ù‡Ù…ØŒ ÙˆØ§Ù„ØªÙŠ ØªØ¹Ø¯ Ù…Ù† Ø£Ø­Ø¯ Ø±Ù…ÙˆØ² Ø§Ù„ÙƒØ±Ù… ÙˆØ§Ù„Ø¶ÙŠØ§ÙØ©",
    "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ù…Ù† Ù‡Ù†Ø§ Ø§Ù†Ø·Ù„Ù‚Øª Ù…ØµØ§Ù†Ø¹ Ù…Ø¹ØªÙˆÙ‚ Ù„ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù‚Ù‡ÙˆØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ù‡Ù„Ø© Ø§Ù„ØªØ­Ø¶ÙŠØ± Ø°Ø§Øª Ø§Ù„Ø±Ø§Ø¦Ø­Ø© Ø§Ù„Ù…Ù…ÙŠØ²Ø© ÙˆØ§Ù„Ø·Ø¹Ù… Ø§Ù„Ø£ØµÙŠÙ„.",
    "Ø§Ù„Ù‚Ù‡ÙˆØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¹ØªÙˆÙ‚ Ù¡Ù©Ù¦Ù ØŒ Ù‚Ù‡ÙˆØ© Ø¹Ø±Ø¨ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø£ØµÙˆÙ„."


Cost



At the time of writing, GPT-4V charges $0.025 per image. By choosing SceneXplainâ€™s MAX plan you pay less than half of that. You can find out more on our pricing page:



SceneXplain - Leading AI Solution for Image Captions and Video Summaries


Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.


SceneXplain


Head to head: GPT-4V vs SceneXplain



Letâ€™s put both services to the test and see who comes out on top.



See our notebook for the full range of testing and results.


Testing methodology



We took a selection of images, some from pexels.com and some from searching Google Images for advertisements in the given languages:



Chinese (we didn't differentiate between Simplified and Traditional)


Japanese


Korean


Arabic


Hindi



We then used:



SceneXplain's API (using the Jelly algorithm)


A combination of the GPT-4V API and web frontend (since we quickly blasted through our API request limit)



We ran several rounds of tests:



Basic image description: For SceneXplain, we just uploaded the image. For GPT-4V, we asked: What is in the image?


Visual question answering: We uploaded the image to both services and asked: What does the text say in this image?


JSON output: For SceneXplain, we simply used the "Extract JSON from image" feature and a predefined JSON Schema.



The JSON Schema was as follows:



{


"type": "object",
  

"properties": {
    

"text_strings": {
      

"type": "array",
      

"description": "Every text string contained in the image. Consider all languages"
    }
  }
}



Since GPT-4V doesn't directly support JSON Schemas, we had to be a bit hacky and explain what we wanted to do as text:



Extract the text strings from this image and populate a JSON that follows this schema:



<JSON Schema from above>



Return just the output JSON. Do not put it in a code block"



The results below focus on the JSON outputs since those are (in our opinion) the most useful outputs for real-world usage.



To save space, weâ€™ve simplified the JSON output to include just the text strings, not the rest of the JSON structure.


Chinese



Weâ€™ve already dived into the McDonaldâ€™s ad above, so weâ€™ll just look at one more Chinese image:



SceneXplain output:


[


"é‡‘é’±è‚š20å…ƒ",
  

"æ—ºè§’ç‰›ç­‹è…©20å…ƒ",
  

"æ—ºè§’ç‰›æ‚18å…ƒ"
]


GPT-4V output:


[


"ç«¹ç­´é­š",
  

"20å…ƒ", 
  

"ç”œç”œåœˆ",
  

"20å…ƒ",
  

"ç”œç”œä»™è²",
  

"18å…ƒ"
]



Again, we see GPT-4V gets the menu contents wrong and also splits each price from the (wrong) menu item. SceneXplain keeps the correct menu items and links them with their prices.



Japanese


GPT-4V gets some of the characters correct in the cigarette ad. The string ç§ã¯ãŸã å¸ã„æ®»ã«ãªã‚Šã¾ã™ has most of the same characters but is still a bit off. Other strings are just hallucinated.


[


"ç§ã¯ãŸã å¸ã„æ®»ã«ãªã‚Šã¾ã™",


"PLAISIR",


"ç´”æ­£",


"MICRONITE",


"æ–°å¼",


"ã™ã¹ã¦ã®å‘³"
]



SceneXplain:


[


    "ç§ã¯ãŸã°ã“ã‚’å¸ã„ã¾ã™",
    "PLAISIR",
    "ç‹¬å ",
    "MICRONITE",
    "æ–°ãŸ",
    "ã‚·ã‚¬ãƒ¬ãƒƒãƒˆ",
    "å–œã³ã®å‘³",
    "KING SIZE"


GPT-4V: After extracting (wrong) text from Chinese and Japanese images, we uploaded another image with the prompt Now do this one (which weâ€™d used successfully to repeat prior commands). GPT-4V seems to have forgotten how. We got the output:


I'm sorry, but I can't assist with identifying or making statements about text within images. If you have any other questions or need assistance with a different task, feel free to ask!



SceneXplain did its duty as expected:


[


    "ç”¨æœãç›´ãšã›èº‡é·¹",
    "ã‚¢ãƒ³ãƒÂ·ãƒ«ãƒ³ã‚²ãƒ³",
    "è©•åˆ¤ã®æ•‘æ€¥è–¬",
    "ã‚¢ãƒ³ãƒãƒ«ãƒ³ãƒ€",
    "è‚ºç‚ã«",
    "æ—¥è‹±ç±³ è£½æ³•ç‰¹è¨±)",
    "ANTI-LUMGEN",
    "ã‚¢ãƒ³ãƒãƒ«ãƒã‚²ãƒ",
    "æ±äº¬éªŒè–¬æ ªå¼ä¼šç¤¾",
    "å‘ˆè´ˆç»æ–‡",
    "(é‡‘ç§‘ç™¾è²¨åº—åŠå„éºµåº— æœ‰ä¸‰)",
    "ã‚ã‚Šã‹ã›ã‚“",
    "æœ¬å‰¤ã¯ã•ãŒå¹´ä»¥ä¸Šä¿å­˜ã—ã¦ã‚‚åŠ¹ç›®ã«è®ŠåŒ–ã¯",
    "æœ¬å‰¤ã¯ä½•ç­‰æœŸä½œå“ã®ãªã„å®‰å…¨ãªå†…æœè–¬ã§ã­",
    "æœ¬å‰¤ã¯ä»–ã®é†«è—¥ã¨ä½µç”¨ã—ã¦å·®æ”¯ã¸ã‚ã‚Šã¾ã›ã‚“",
    "ã®å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“",
    "æœ¬åŠ‘ã¯ã€Œåªä¸€å›åˆ†ã€ã®ã¿ã¦çµæ§‹ã§ã™æˆ˜ç‰ˆ",
    "ã‚‰å°‚é–€å®¶ã«æœŸç›¸è«‡ä¸‹ã•ã„",
    "è±¡ã®è¶…ã‚‰ãªã„æ™‚ã¯ä¹¾ç‚ã§ãªã„ã¨æ€ã²ã¾ã™",
    "ã‚¢ãƒ³ãƒÂ·ãƒ«ãƒ³ã‚²ãƒ³ ã‚’æœç”¨ã—ã¾ã™ã¨å¿…ãšå³ã®",
    "æœ¬åŠ‘ ã® ç‰¹ é•·",


    "ã‚‹ã®ã§ã‚ã‚Šã¾ã—ã¦å®‰çœ ã‚ˆã‚Šè¦ºã‚ã‚‹æ™‚ã«ã¯å¾³",
    "æ­¤ã®ç‰¹æ®Šç¾è±¡ ç™¼æ±—å®‰çœ  ã“ãè–¬åŠ¹ã§æ’²æ»…",
    "ã¨ã‚‚å¿«ä¸Šã’ã«é•·æ™‚é–“å®‰çœ è‡´ã—ã¾ã™",
    "ç™¼æ±—ã—å…¶ç™¼æ±—ãŒç›¸ä¹‹æ•™ç”Ÿã‚‹é ƒã‚ˆã‚Šå¿ è€…ã¯ã„",
    "ã‚¢ãƒ³ãƒÂ·ãƒ«ãƒ³ã‚²ãƒ³ ã‚’æœç”¨ã™ã‚‹ã¨ç™½è¡€çƒã®ã¯",
    "æ²» ç™‚ ã® éµ å‰‡",
    "å¦‚ä¸–æ¦®79ç™¼éŸ³å…ƒä¸­å±±",
    "åƒ¹ å°äººç›¸ é‡‘ åœ‹",
    "å¤§äººç¿” äº”",
    "éƒ¨å­¸åŒ–æ‰€é‹¼è£½å±±ä¸­ ç¤¾æœƒå¼æ ¼ å…ƒè³£ç™¼",
    "ç¤¾æœƒè€…åˆæœ¨å¤§ Â·åº—å•†ç½®ç½ æ„ æŸ)",
    "ç¤¾æœƒåä»¤éƒ¨å¤ªé•·å·é¾ åœ‹å¤å(",
    "æœƒé¢ã‚¤ãƒ©ãƒ–ã‚µãƒ«ã‚¿ãƒ“ã‚¹ãƒ› èªª å¤§ãƒ",
    "åº—ç†ä»£",
    "ç¤¾æœƒå¼æ ªè—¥è£½äº¬æ± å…ƒé€ è£½",
    "Image ID: W5DPKC",
    "www.alamy.com"


Korean


GPT-4V remembers how to extract text again. And it remembers how to get it wrong too! If you look at the output, some of the characters are the same as the image, but some are not.


[


"ìˆ ì€ ë‚¨ì— ë§ë‚˜",


"ìˆ ì€ ë‚˜ë¡œ ë§ë‚˜"
]



SceneXplain gives accurate output, though Iâ€™m not sure where the 50 is coming from:


[


    "|ì•¼",
    "ì¢‹ì€ ë‚ ì— ë§Œë‚˜",
    "ë‹¨ì´ ë°©",
    "50",
    "ì¢‹ì€ ë‚˜ë¥¼ ë§Œë‚˜"


GPT-4V actually does okay with the more complex image. It doesnâ€™t catch all the strings, but itâ€™s less wrong than usual from a quick comparison (note: I donâ€™t read Korean, so Iâ€™m just eyeballing it):


[


"ìì—°ê³¼ êµ­ë¯¼ì„ ì—°ê²°í•˜ëŠ”",


"ìˆ˜ëª©Â·ì •ì› í”Œë«í¼ ê¸°ê´€",


"ìˆ˜ëª©ì›ì˜ ê³ ê°ê°€ì¹˜ë¥¼ ë„˜ì–´",


"êµ­ë¯¼ ìƒí™œ ì† ë…¹ìƒ‰ê³µê°„ í™•ì‚°ê³¼",


"ì •ì›ì‚°ì—… ìƒíƒœê³„ë¥¼ í™œì„±í™”í•˜ëŠ” ì—­í• ì„",


"ì ê·¹ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê² ìŠµë‹ˆë‹¤"
]



SceneXplain extracts a lot more:


[


    "í•œêµ­ìˆ˜ëª©ì›ì •ì›ê´€ë¦¬ì›",
    "ìì—°ê³¼ êµ­ë¯¼ì„ ì—°ê²°í•˜ëŠ”",
    "ìˆ˜ëª©ì›Â·ì •ì› í”Œë«í¼ ê¸°ê´€",
    "ìˆ˜ëª©ì›ì˜ ê³µê°„ì  ê°œë…ì„ ë„˜ì–´",
    "êµ­ë¯¼ ìƒí™œ ì† ë…¹ìƒ‰ë¬¸í™” í™•ì‚°ê³¼",
    "ì •ì›ì‚°ì—… ìƒíƒœê³„ë¥¼ í™œì„±í™”í•˜ëŠ” ì—­í• ì„",
    "ì ê·¹ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê² ìŠµë‹ˆë‹¤",
    "ì‚°ë¦¼ìƒë¬¼ìì›",
    "ê³ í’ˆì§ˆ ìˆ˜ëª©ì›",
    "ES",
    "ë³´ì „ Â· ë³µì›",
    "ì„œë¹„ìŠ¤",
    "ì—­ëŸ‰ ê°•í™”",
    "ì‚°ë¦¼ìƒë¬¼ìì›",
    "ì—°êµ¬ Â·í™œìš©",
    "ì •ì›ë¬¸í™”",
    "í™œì„±í™”"


Arabic



Since we already covered the Arabic coffee image above, we'll just leave one Arabic here:



GPT-4V:


[


"Ø§Ù„Ø­ÙŠØ§Ø© Ø·Ø±ÙŠÙ‚ Ø£ÙƒØ«Ø± Ø³Ø±Ø¹Ø©"
]



SceneXplain gets a little confused about the 4G logo, seeing it as EAG but is otherwise sound:


[


    "Ø§Ù„Ø­ÙŠØ§Ø© ØµØ§Ø±Øª Ø£ÙƒØ«Ø± Ø³Ø±Ø¹Ø©",
    "EAG",
    "LTE"


Hindi


GPT-4V:


[


"à¤¸à¤°à¤¸à¥‹à¤‚ à¤”à¤° à¤†à¤‚à¤µà¤²à¤¾",


"à¤•à¤¾ à¤ªà¥‹à¤·à¤£ à¤¬à¤¿à¤¨à¤¾ à¤šà¤¿à¤ªà¤šà¤¿à¤ªà¤¾à¤¹à¤Ÿ",


"à¤¸à¤¿à¤°à¥à¤« à¤°à¥",


"9 à¤®à¥‡à¤‚",


"40ml",


"à¤¡à¤¾à¤¬à¤°",


"à¤Ÿà¥‹à¤² à¤«à¥à¤°à¥€ 1800-103-1644"
]



SceneXplain:


[


    "à¤¸à¤°à¤¸à¥‹à¤‚ à¤†à¤à¤µà¤²à¤¾",
    "à¤•à¥‡à¤¶ à¤¤à¥‡à¤²",
    "à¤¸à¤°à¤¸à¥‹à¤‚ à¤”à¤° à¤†à¤à¤µà¤²à¤¾",
    "à¤•à¤¾ à¤ªà¥‹à¤·à¤£ à¤¬à¤¿à¤¨à¤¾",
    "à¤šà¤¿à¤ªà¤šà¤¿à¤ªà¤¾à¤¹à¤Ÿ",
    "à¤®à¥‡à¤‚",
    "à¤¨à¤¯à¤¾",
    "à¤ªà¥ˆà¤•",
    "â‚¹9/-",
    "à¤¸à¤°à¤¸à¥‹à¤‚ à¤†à¤à¤µà¤²à¤¾",
    "40ml",
    "DABUR CARES: CALL OR WRITE",
    "+ TOLL FREE 1800-103-1644"


GPT-4V gave us the error Something went wrong. If this issue persists please contact us through our help center at help.openai.com. After retrying, it gave us:


[


"MDH à¤•à¥€ à¤à¤•",


"à¤§à¤®à¤¾à¤•à¥‡à¤¦à¤¾à¤° à¤‘à¤«à¤°",


"à¤¸à¥à¤µà¤¾à¤¦ à¤•à¥‡ à¤¸à¤¾à¤¥ - à¤¸à¤¾à¤¥ à¤–à¥à¤¶à¤¿à¤¯à¥‹à¤‚ à¤•à¥€ à¤¬à¥Œà¤›à¤¾à¤° !!",


"à¤®à¤¸à¤¾à¤²à¤¾ à¤à¤• à¤¸à¥à¤µà¤¾à¤¦à¤¿à¤·à¥à¤Ÿ à¤”à¤° à¤œà¥€à¤µà¤‚à¤¤ à¤‰à¤ªà¤¹à¤¾à¤° !!",


"MDH",


"à¤®à¤¸à¤¾à¤²à¥‡",


"à¤…à¤¸à¤²à¥€ à¤®à¤¸à¤¾à¤²à¥‡ à¤¸à¤š-à¤¸à¤š",


"MDH Ltd.",


"E-mails: delhi@mdhspices.in, rk@mdhspices.in www.mdhspices.com"
]



SceneXplain goes much further, even extracting product names and tiny details like when the company was established:


[


    "MDH",
    "à¤•à¥€ à¤à¤•",
    "à¤§à¤®à¤¾à¤•à¥‡à¤¦à¤¾à¤° à¤‘à¤«à¤°",
    "à¤¸à¥à¤µà¤¾à¤¦ à¤•à¥‡ à¤¸à¤¾à¤¥ -à¤¸à¤¾à¤¥ à¤–à¥à¤¶à¤¿à¤¯à¥‹à¤‚ à¤•à¥€ à¤¬à¥Œà¤›à¤¾à¤° !",
    "à¤®à¤¸à¤¾à¤²à¤¾ à¤ªà¥ˆà¤• à¤–à¤°à¥€à¤¦à¥‹ à¤”à¤° à¤œà¥€à¤¤à¥‹ à¤‰à¤ªà¤¹à¤¾à¤° !!",
    "à¤à¤® à¤¡à¥€ à¤à¤š à¤•à¥€ à¤“à¤° à¤¸à¥‡ à¤…à¤ªà¤¨à¥‡ à¤¸à¤­à¥€ à¤—à¥à¤°à¤¾à¤¹à¤•à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤à¤• à¤§à¤®à¤¾à¤•à¥‡à¤¦à¤¾à¤° à¤‘à¤«à¤°à¥¤",
    "à¤•à¤¿à¤¸à¥€ à¤­à¥€ à¤¨à¤œà¤¦à¥€à¤•à¥€ à¤µà¤¿à¤•à¥à¤°à¥‡à¤¤à¤¾ à¤¸à¥‡ à¤à¤® à¤¡à¥€ à¤à¤š à¤®à¤¸à¤¾à¤²à¥‹à¤‚ à¤•à¥‡ à¤¨à¥€à¤šà¥‡ à¤›à¤ªà¥‡ à¤ªà¥ˆà¤•à¥à¤¸ à¤®à¥‡à¤‚ à¤¸à¥‡",
    "à¤•à¥‹à¤ˆ à¤­à¥€ 100 à¤—à¥à¤°à¤¾à¤® à¤µà¤¾à¤²à¤¾ à¤ªà¥ˆà¤• à¤–à¤°à¥€à¤¦à¥‡à¤‚ à¤”à¤° à¤­à¤¾à¤—à¥à¤¯à¤¶à¤¾à¤²à¥€ à¤µà¤¿à¤œà¥‡à¤¤à¤¾ à¤¬à¤¨à¤¨à¥‡ à¤•à¤¾ à¤…à¤µà¤¸à¤°",
    "à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤•à¤°à¥‡à¤‚, à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ à¤†à¤ª à¤•à¥‡ à¤²à¤¿à¤ à¤—à¤ à¤ªà¥ˆà¤• à¤•à¥‡ à¤«à¥à¤²à¥‡à¤ª à¤®à¥‡à¤‚ à¤‰à¤ªà¤¹à¤¾à¤° à¤•à¤¾ à¤¨à¤¾à¤® à¤¹à¥‹à¥¤",
    "à¤…à¤¸à¤²à¥€ à¤®à¤¸à¤¾à¤²à¥‡",
    "à¤®à¤¸à¤¾à¤²à¥‡",
    "à¤¸à¤š -à¤¸à¤š",
    "MD H",
    "SPICES",
    "MD",
    "MOH",
    "Garam",
    "Pav Bhaji",
    "masala",
    "Kitchen",
    "Shahi Pancer",


    "King",
    "Chana",
    "DEGGI",
    "MIRCH",
    "Chunky Chat",
    "à¤¨à¤¿à¤¯à¤® à¤µ à¤¶à¤°à¥à¤¤à¥‡à¤‚ :- à¤à¤® à¤¡à¥€ à¤à¤š à¤®à¤¸à¤¾à¤²à¥‹à¤‚ à¤•à¥‡ à¤Šà¤ªà¤° à¤¦à¤°à¥à¤¶à¤¾à¤¯à¥‡ à¤—à¤ à¤šà¥à¤¨à¤¿à¤¨à¥à¤¦à¤¾ à¤ªà¥ˆà¤•à¥à¤¸ à¤®à¥‡à¤‚ à¤¸à¥‡ à¤•à¥‹à¤ˆ à¤­à¥€ 100 à¤—à¥à¤°à¤¾à¤® à¤µà¤¾à¤²à¤¾",
    "à¤ªà¥ˆà¤• à¤–à¤°à¥€à¤¦à¥‡à¤‚, à¤‰à¤¸à¤•à¤¾ à¤«à¥à¤²à¥‡à¤ª à¤–à¥‹à¤²à¥‡à¤‚, à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤‰à¤¸ à¤«à¥à¤²à¥‡à¤ª à¤•à¥‡ à¤¨à¥€à¤šà¥‡ à¤‰à¤ªà¤¹à¤¾à¤° à¤•à¤¾ à¤¨à¤¾à¤® à¤›à¤ªà¤¾ à¤¹à¥‹à¥¤ à¤‰à¤¸",
    "à¤‰à¤ªà¤¹à¤¾à¤° à¤•à¥‹ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤­à¤¾à¤—à¥à¤¯à¤¶à¤¾à¤²à¥€ à¤µà¤¿à¤œà¥‡à¤¤à¤¾ à¤¬à¤¨à¥‡à¤‚à¥¤ â€¢ à¤‡à¤¸ à¤¸à¥à¤•à¥€à¤® à¤•à¤¾ à¤¨à¤•à¤¦ à¤²à¤¾à¤­ à¤•à¥‹à¤ˆ à¤­à¥€ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆ",
    "à¤²à¤¿à¤ à¤¦à¤¿à¤²à¥à¤²à¥€ à¤®à¥‡à¤‚ à¤¹à¥€ à¤²à¤¾à¤—à¥‚ à¤¹à¥ˆ â€¢ à¤¸à¥à¤•à¥€à¤® à¤¸à¥à¤Ÿà¥‰à¤• à¤°à¤¹à¤¨à¥‡ à¤¤à¤• à¤¸à¥€à¤®à¤¿à¤¤ à¤¹à¥ˆ â€¢ à¤¯à¤¹ à¤¸à¥à¤•à¥€à¤® à¤à¤® à¤¡à¥€ à¤à¤š à¤•à¥‡ à¤¸à¥à¤ªà¤°",
    "à¤¸à¥à¤Ÿà¥‰à¤•à¤¿à¤¸à¥à¤Ÿ, à¤¸à¥à¤Ÿà¥‰à¤•à¤¿à¤¸à¥à¤Ÿ à¤”à¤° à¤•à¤°à¥à¤®à¤šà¤¾à¤°à¤¿à¤¯à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆ Â· à¤•à¤‚à¤ªà¤¨à¥€ à¤•à¤¾ à¤¨à¤¿à¤°à¥à¤£à¤¯ à¤…à¤‚à¤¤à¤¿à¤® à¤”à¤° à¤¸à¤°à¥à¤µà¤®à¤¾à¤¨à¥à¤¯ à¤¹à¥‹à¤—à¤¾",
    "à¤”à¤° à¤¸à¤­à¥€ à¤µà¤¿à¤µà¤¾à¤¦ à¤¦à¤¿à¤²à¥à¤²à¥€ à¤•à¥‡ à¤¨à¥à¤¯à¤¾à¤¯ à¤•à¥à¤·à¥‡à¤¤à¥à¤° à¤•à¥‡ à¤…à¤§à¥€à¤¨ à¤¹à¥‹à¤‚à¤—à¥‡à¥¤ â€¢ à¤…à¤¨à¥à¤¯ à¤¶à¤°à¥à¤¤à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤ªà¥ˆà¤• à¤¦à¥‡à¤–à¥‡à¤‚à¥¤",
    "à¤®à¤¹à¤¾à¤¶à¤¿à¤¯à¤¾à¤ à¤¦à¥€ à¤¹à¤Ÿà¥à¤Ÿà¥€ (à¤ªà¥à¤°à¤¾à¥¦) à¤²à¤¿à¤®à¤¿à¤Ÿà¥‡à¤¡",
    "9/44, à¤•à¥€à¤°à¥à¤¤à¤¿ à¤¨à¤—à¤°, à¤¨à¤ˆ à¤¦à¤¿à¤²à¥à¤²à¥€ - 110015 à¤«à¥‹à¤¨ à¤¨à¤‚à¥¦ 011-41425106 - 07-08",
    "ESTD. 1919",
    "E-mails : delhi@mdhspices.in, rk@mdhspices.in www.mdhspices.com"


Conclusion



At the end of the day, SceneXplain clearly excels in multilingual OCR compared to GPT-4V. To recap:



API limits:



GPT-4V: 100 requests per day


SceneXplain: 5,000 credits per month on the MAX plan. JSON output with Jelly costs 2 credits, so you can process 2,500 images per month.



Reliability:



GPT-4V: Is often like â€œIâ€™m sorry Dave, Iâ€™m afraid I canâ€™t do that.â€


SceneXplain: Doggedly reliable. Gets the job done.



Accuracy:



GPT-4V: hallucinates strings it believes are plausible from a given image, rather than reading the actual text.


SceneXplain: Extracts the correct text.



Consistency:



GPT-4V doesnâ€™t just hallucinate. It hallucinates different things each time.


SceneXplain gives much more consistent output.



Completeness:



GPT-4V: Often doesnâ€™t always extract all the strings in the image.


SceneXplain: Extracts more strings.



Cost:



GPT-4V: $0.025 per image.


SceneXplain: As low as 0.02 USD per image on the MAX plan if you enable JSON output. As low as 0.01 USD without JSON output.



When you take all those factors into account, it's clear that SceneXplain is the obvious choice.



Get started with SceneXplain



Ready to explore multilingual OCR? Sign up for a free account with SceneXplain and start converting images to text easily. It's a practical, user-friendly way to manage digital documents. Get started now and see the difference SceneXplain can make!



SceneXplain - Leading AI Solution for Image Captions and Video Summaries


Experience cutting-edge computer vision with our premier image captioning and video summarization algorithms. Tailored for content creators, media professionals, SEO experts, and e-commerce enterprises. Featuring multilingual support and seamless API integration. Elevate your digital presence today.


SceneXplain


Categories:


Tech blog


rss_feed


Top-5 similar articles


play_arrow


GET TOP-5


Select reranker


Read more


August 26, 2024 â€¢ 13 minutes read


The What and Why of Text-Image Modality Gap in CLIP Models


You can't just use a CLIP model to retrieve text and images and sort the results by score. Why? Because of the modality gap. What is it, and where does it come from?


August 22, 2024 â€¢ 8 minutes read


Late Chunking in Long-Context Embedding Models


Chunking long documents while preserving contextual information is challenging. We introduce the "Late Chunking" that leverages long-context embedding models to generate contextual chunk embeddings for better retrieval applications.


July 31, 2024 â€¢ 17 minutes read


Rephrased Labels Improve Zero-Shot Text Classification by 30%


When using embedding models for zero-shot classification, rephrasing the class label to "This is seriously about 'LABEL'" gives higher accuracy vs. using LABEL alone. But how, and why?


OFFICES


location_on


Berlin, Germany (HQ)


PrinzessinnenstraÃŸe 19-20, 10969 Berlin, Germany


GeschÃ¤ftsanschrift: Leipzigerstr. 96, 10117 Berlin, Germany


location_on


Beijing, China


Level 5, Building 6, No.48 Haidian West St. Beijing Haidian, China


location_on


Shenzhen, China


402, Floor 4, Fu'an Technology Building, Shenzhen Nanshan, China


SEARCH FOUNDATION


Embeddings


Reranker


Reader


Segmenter


Get Jina AI API key


API Status


COMPANY


About us


Contact sales


Newsroom


Intern program


Join us


open_in_new


Download logo


open_in_new


TERMS


Terms & Conditions


Privacy


Manage Cookies


email


language


English


science


Jina AI GmbH Â© 2020-2024.

---